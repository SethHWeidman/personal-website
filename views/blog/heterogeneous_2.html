<link rel="stylesheet" href="/fraud-tables.css" />

<h1 class="post-title">
  Building Fraud Models with Heterogeneous Labels of Unknown Quality (draft)
</h1>
<p class="post-subtitle">
  How We Built a Model-Backed First Party Fraud Product at SentiLink
</p>
<p class="post-date">January 22, 2026</p>

<p>
  This essay is about how we built a 0-to-1 first party fraud (FPF) risk score at
  SentiLink using a machine learning model, despite having to train on heterogeneous
  label sets whose quality and relevance to the fraud problem we could not directly
  verify. The problem was the broad category of “first party fraud,” a collection of
  fraud variants that our customers (financial institutions offering lending and
  depository products) typically discovered only after the fact. They would observe
  unusually bad behavior from one of <em>their</em> customers and, possibly after manual
  review, conclude that first party fraud had taken place at the time of the application.
</p>

<p>
  As a result, our labels were an amalgam of heterogeneous case sets: many where partners
  observed behaviors highly suggestive of FPF, and some where partners reached an FPF
  determination through manual review. Examples included accounts where bad checks were
  passed, accounts with large customer-initiated ACH returns, and cases where a partner
  determined that a customer had charged off and never had intent to pay (possibly
  because they planned to engage in credit washing).
</p>

<p>
  This created three challenges for building a score. First, the labels were obviously
  heterogeneous. Second, there was mislabeling risk: some labels, especially those based
  primarily on observing a bad outcome, may not have been FPF at all (the behavior could
  have been due to identity theft, for example). Third, during certain fraud attacks a
  broad swath of applications might be labeled bad even though some of those applications
  were not fraudulent.
</p>

<p>
  These properties were new for SentiLink to deal with. When we started working on first
  party fraud scores in mid 2022, SentiLink had two successful products: scores to help
  financial institutions stop synthetic fraud and identity theft. The company’s founders
  had built solutions to address these problems at Affirm, the online lender, and had
  strong priors on the building blocks needed to enable machine learning models tailored
  to them. We licensed data sources relevant to the patterns associated with synthetic
  fraud and identity theft; used these sources, along with first-party data collected via
  our APIs, to build data structures relevant to detecting these fraud M.O.s; built
  internal tooling to visualize these structures; and hired an internal team of fraud
  analysts (several of whom came from Affirm) to use this tooling to label subsets of
  applications our partners sent us.
</p>

<p>
  These curated labels, each based on manual review of on-prem data relevant to the
  application, were then used by the data science team to train our models. This approach
  carried the company from shortly after founding (when the decision to focus on “scores”
  was made) through
  <a href="https://www.craftventures.com/articles/why-we-invested-in-sentilink"
    >the Series B fundraise from Craft Ventures in May 2021</a
  >.
</p>

<h2>Our FPF problem and an analogy to understand it</h2>

<p>
  With first party fraud, we could not determine through manual review that an individual
  application was FPF in the same way we could for synthetic fraud and identity theft.
  The main reason is that FPF is typically discovered through unusually bad behavior by a
  customer of the financial institution.
</p>

<p>
  Two other reasons relate to the definitions of the fraud types themselves and the data
  we had on prem. First, synthetic fraud and identity theft can often be detected via
  “mismatches” in PII on the application: for example, a brand new social security number
  for someone in their 40s can be a sign of synthetic fraud (specifically
  <em>first party</em> synthetic fraud), and an address, phone number, and social
  security number from different states can be a sign of identity theft. Second, the data
  we licensed and the data structures we built were highly tailored toward detecting
  synthetic fraud and identity theft. Two examples:
</p>

<ul>
  <li>
    We had an “identity database,” based mostly on licensed data, that was structured in
    a way that more or less “natively” revealed first party synthetic fraud (where one
    person uses multiple SSNs with their own name and date of birth), an extremely common
    synthetic fraud variant.
  </li>
  <li>
    We had an “application graph,” based mostly on first-party data (for example, our
    customers calling our API), that let us see whether one phone number was being used
    on multiple applications with different pieces of core PII, a sign that it might be
    the phone of a fraudster who had stolen multiple identities.
  </li>
</ul>

<p>
  So we <em>had</em> to rely on labels from our partners, with the heterogeneity and
  accuracy issues described above.
</p>

<p>
  An analogy: suppose your task is to train a classifier to detect whether a car is in an
  image, given several sets of images handed to you by individuals of varying expertise.
  Maybe one person gives you two chunks (one of 1,000 images and another of 2,000
  images), and a second person gives you a chunk of 3,000 images, and so on. However,
  <em
    >looking at the images yourself would not actually confirm whether a car is in the
    image</em
  >. While most chunks presumably contain cars, some may instead contain motorcycles,
  some may contain trucks, and some chunks may not even be images of vehicles at all.
</p>

<p>
  How do you proceed? Suppose you can manually define certain “<strong>patterns</strong>”
  and check whether each pattern is present in each image. For example, you can detect
  whether there is a sheet of metal (which could be part of a door), whether there is a
  pane of glass (which could be a window or windshield), whether there is a metal circle
  inside a rubber circle (which could indicate a wheel), and so on. You can also define
  patterns that might be present in an image containing a motorcycle or a truck
  (handlebars, a large cab), but not in one containing a car.
</p>

<p>
  This information, even taken together, is not sufficient to determine whether a set of
  images contains cars. But from the prevalence of these patterns among the images, and
  by comparing that prevalence to the prevalence among a random set of internet images,
  you can infer which chunks are <em>likely</em> to contain cars and therefore are likely
  to be beneficial if included as “targets” for the “car” model.
</p>

<p>
  For us, the analogues to these “material patterns” were patterns we could observe in
  the data tied to applications: in the identity database, the graph, and other on-prem
  data structures. We hypothesized that these patterns could serve as signals for first
  party fraud.
</p>

<p>
  There was a virtually infinite universe of potential signals we could engineer. We
  refined the hypothesis space through:
</p>

<ul>
  <li>
    <strong>Primary research into first party fraud M.O.s.</strong> A common type of FPF
    involving ACH transactions requires multiple checking accounts, so we hypothesized
    that applying for multiple checking accounts in a short period might be predictive.
    We could observe this behavior in the application graph, even though it was built to
    identify identity theft.
  </li>
  <li>
    <strong>Reviewing cases from the ambiguous labels we received.</strong> Even though
    the labels were imperfect, we repeatedly saw certain anomalous patterns. For example,
    we could see in our identity database that a disproportionately high number of the
    supposed first party fraudsters had <em>previously</em> committed synthetic fraud,
    even if they were not committing synthetic fraud on the specific applications labeled
    as first party fraud.
  </li>
</ul>

<p>
  Each of these could plausibly be a signal for first party fraud. We still had to
  validate them quantitatively using the heterogeneous, unreliable labels from customers.
  The next section describes how we did that.
</p>

<h2>Evaluating hypothesized signals on heterogeneous, unreliable labels</h2>

<p>Suppose we started with three sets of potential first party fraud labels:</p>

<ul>
  <li>
    One from a credit union, with 4,000 overall “first party fraud” labels divided into
    three subcategories.
  </li>
  <li>
    One from a large bank, consisting of a single chunk of 2,000 first party fraud
    labels.
  </li>
  <li>
    One from another large bank, with 18,000 first party fraud labels divided into two
    subcategories.
  </li>
</ul>

<p>
  For each dataset, we produced a table showing how each potential signal behaved on both
  the fraud labels and the relevant population from which those applications were drawn:
</p>

<!-- Credit Union 1 table -->
<div class="fraud-table-container">
  <h3>Credit Union 1</h3>
  <div class="fraud-table-scroll">
    <table id="credit-union-1-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>4,000</td>
          <td class="highlight">32.9%</td>
          <td class="highlight">39.0%</td>
          <td>2.6%</td>
          <td>3.0%</td>
          <td>4.4%</td>
          <td>1.3%</td>
          <td>1,317</td>
          <td>1,560</td>
          <td>105</td>
          <td>119</td>
          <td>177</td>
          <td>50</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>2,500</td>
          <td class="highlight">37.9%</td>
          <td class="highlight">30.0%</td>
          <td>3.3%</td>
          <td>1.4%</td>
          <td>1.3%</td>
          <td>1.2%</td>
          <td>948</td>
          <td>749</td>
          <td>82</td>
          <td>36</td>
          <td>33</td>
          <td>30</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>1,000</td>
          <td>3.8%</td>
          <td>3.8%</td>
          <td>2.8%</td>
          <td>1.7%</td>
          <td>2.7%</td>
          <td>2.8%</td>
          <td>38</td>
          <td>38</td>
          <td>28</td>
          <td>17</td>
          <td>27</td>
          <td>28</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_3</td>
          <td>500</td>
          <td class="highlight">32.8%</td>
          <td class="highlight">22.8%</td>
          <td>3.2%</td>
          <td>4.0%</td>
          <td>2.4%</td>
          <td>2.6%</td>
          <td>164</td>
          <td>114</td>
          <td>16</td>
          <td>20</td>
          <td>12</td>
          <td>13</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>200,000</td>
          <td>4.2%</td>
          <td>2.8%</td>
          <td>4.0%</td>
          <td>3.8%</td>
          <td>1.5%</td>
          <td>1.7%</td>
          <td>8,422</td>
          <td>5,676</td>
          <td>7,908</td>
          <td>7,581</td>
          <td>3,096</td>
          <td>3,368</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!-- Large Bank 1 table -->
<div class="fraud-table-container">
  <h3>Large Bank 1</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-1-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>2,000</td>
          <td>4.3%</td>
          <td class="highlight">15.1%</td>
          <td class="highlight">21.2%</td>
          <td class="highlight">27.4%</td>
          <td>1.2%</td>
          <td>3.2%</td>
          <td>86</td>
          <td>302</td>
          <td>425</td>
          <td>547</td>
          <td>24</td>
          <td>63</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>165,000</td>
          <td>4.4%</td>
          <td>2.1%</td>
          <td>2.5%</td>
          <td>3.6%</td>
          <td>4.3%</td>
          <td>3.4%</td>
          <td>7,179</td>
          <td>3,469</td>
          <td>4,192</td>
          <td>5,981</td>
          <td>7,076</td>
          <td>5,578</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!-- Large Bank 2 table -->
<div class="fraud-table-container">
  <h3>Large Bank 2</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-2-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>18,000</td>
          <td>3.5%</td>
          <td>3.4%</td>
          <td>3.5%</td>
          <td>1.1%</td>
          <td>1.9%</td>
          <td>3.8%</td>
          <td>622</td>
          <td>606</td>
          <td>639</td>
          <td>200</td>
          <td>350</td>
          <td>683</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>12,500</td>
          <td>4.1%</td>
          <td>3.4%</td>
          <td>2.2%</td>
          <td>3.2%</td>
          <td>3.9%</td>
          <td class="highlight">17.8%</td>
          <td>513</td>
          <td>419</td>
          <td>269</td>
          <td>405</td>
          <td>482</td>
          <td>2,227</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>5,500</td>
          <td class="highlight">27.6%</td>
          <td class="highlight">30.3%</td>
          <td>4.5%</td>
          <td>2.0%</td>
          <td>2.9%</td>
          <td>4.0%</td>
          <td>1,517</td>
          <td>1,666</td>
          <td>248</td>
          <td>109</td>
          <td>160</td>
          <td>218</td>
        </tr>
        <tr>
          <td>overall_apps</td>
          <td>240,000</td>
          <td>2.6%</td>
          <td>8.2%</td>
          <td>8.9%</td>
          <td>8.4%</td>
          <td>2.6%</td>
          <td>5.8%</td>
          <td>2,580</td>
          <td>8,239</td>
          <td>8,947</td>
          <td>8,474</td>
          <td>2,753</td>
          <td>5,818</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<h2>What these tables tell us</h2>

<p>
  These tables show, for each chunk of fraud labels, the percent of labels flagged by
  each signal (the signal’s “recall” on that chunk). For comparison, the bottom row of
  each table shows the percent of the overall population flagged by the same signal.
</p>

<p>
  The ratio of these numbers was a key quantity for us. We called it “relative
  likelihood”:
</p>

<pre><code>relative_likelihood =
  P(signal = 1 | label = fraud) / P(signal = 1)</code></pre>

<p>
  This is a proxy for precision that we would look at internally and present alongside
  recall. We found it easier for potential customers to translate “This feature flags 30%
  of your fraud while flagging 2% of your approvals, a 15x ratio” into business value
  than “This feature has 30% recall and 15% precision.”
  <sup id="fnref3"><a href="#fn3">3</a></sup>
</p>

<!-- Credit Union 1 relative likelihood table -->
<div class="fraud-table-container">
  <h3>Credit Union 1 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="credit-union-1-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>4,000</td>
          <td class="highlight">7.83</td>
          <td class="highlight">13.93</td>
          <td>0.65</td>
          <td>0.79</td>
          <td>2.93</td>
          <td>0.76</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>2,500</td>
          <td class="highlight">9.02</td>
          <td class="highlight">10.71</td>
          <td>0.82</td>
          <td>0.37</td>
          <td>0.87</td>
          <td>0.71</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>1,000</td>
          <td>0.90</td>
          <td>1.36</td>
          <td>0.70</td>
          <td>0.45</td>
          <td>1.80</td>
          <td>1.65</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_3</td>
          <td>500</td>
          <td class="highlight">7.81</td>
          <td class="highlight">8.14</td>
          <td>0.80</td>
          <td>1.05</td>
          <td>1.60</td>
          <td>1.53</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>200,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!-- Large Bank 1 relative likelihood table -->
<div class="fraud-table-container">
  <h3>Large Bank 1 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-1-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>2,000</td>
          <td>0.98</td>
          <td class="highlight">7.19</td>
          <td class="highlight">8.48</td>
          <td class="highlight">7.61</td>
          <td>0.28</td>
          <td>0.94</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>165,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!-- Large Bank 2 relative likelihood table -->
<div class="fraud-table-container">
  <h3>Large Bank 2 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-2-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>18,000</td>
          <td>1.35</td>
          <td>0.41</td>
          <td>0.39</td>
          <td>0.13</td>
          <td>0.73</td>
          <td>0.66</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>12,500</td>
          <td>1.58</td>
          <td>0.41</td>
          <td>0.25</td>
          <td>0.38</td>
          <td>1.50</td>
          <td class="highlight">3.07</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>5,500</td>
          <td class="highlight">10.62</td>
          <td class="highlight">3.70</td>
          <td>0.51</td>
          <td>0.24</td>
          <td>1.12</td>
          <td>0.69</td>
        </tr>
        <tr>
          <td>overall_apps</td>
          <td>240,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p>
  You’ll notice that we also included recalls and relative likelihoods for scoring high
  on our synthetic fraud and identity theft models. Extending the cars/trucks/motorcycles
  analogy above: in addition to detecting material patterns, imagine you also had models
  that were 80% to 90% accurate at detecting whether an image contained a truck or a
  motorcycle.
</p>

<h3>Making sense of these tables</h3>

<p>Even from these six tables, several things jump out:</p>

<ul>
  <li>
    Signals 1 and 2 tend to “pop” on sub-categories 1 and 3 from the credit union and
    sub-category 2 from the second large bank. These signals often fire together, and
    they tend to fire on distinct sets of fraud-label datasets.
  </li>
  <li>
    Signal 1 does not meaningfully pop on Large Bank 1 overall (relative likelihood near
    1), whereas Signals 2 to 4 do.
  </li>
  <li>
    Sub-category 1 for the second large bank has a high rate of identity theft (measured
    by the fraction of these applications that score highly on our identity theft score),
    which suggested to us that this label set was likely mostly identity theft rather
    than first party fraud.
  </li>
  <li>
    Sub-category 2 from the credit union did not fire on any hypothesized FPF signals,
    suggesting either a different underlying M.O. or substantial label noise.
  </li>
</ul>

<h2>From signals to scores</h2>

<p>
  This analysis helped us understand our data deeply. But how did we go from these
  numbers to first party fraud scores?
</p>

<p>
  Before making technical decisions like the model class (logistic regression vs.
  gradient-boosted trees) or hyperparameter selection, there are decisions closer to
  business decisions that often have a much larger impact on model efficacy:
</p>

<ul>
  <li>
    Which features (or “signals,” as we tended to call them) to use, meaning what the
    columns of the modeling dataset will be.
  </li>
  <li>
    Which dataset or datasets to train on, including choices about both the “bads” and
    the “not bads.”
  </li>
</ul>

<p>
  <strong>
    The analysis above directly informed both questions. Without breaking down how key
    signals are (or are not) associated with candidate targets, you are largely taking
    shots in the dark. An alternative is to feed every conceivable signal into the model.
    We learned hard lessons doing this with our synthetic fraud and identity theft
    models: while it did not necessarily degrade aggregate model performance, it produced
    less explainable SHAP values and occasional misses that raised uncomfortable
    questions among our largest and most conservative customers.
  </strong>
</p>

<p>
  Of the 10 to 20 potential signals we evaluated, we ultimately found eight that were
  conceptually distinct and individually predictive on many of the datasets we received.
  By this we mean they were prevalent among many FPF label datasets and not so prevalent
  overall, which corresponds to high recall and high relative likelihood, respectively.
  Each of these eight signals was a boolean flag, accompanied by numeric sub-signals (for
  example, the number of distinct SSNs someone had previously used for synthetic fraud,
  or the number of DDAs applied for across different time windows). These signals became
  about 40 features in the model.
</p>

<p>
  To choose which datasets to use as “goods” and “bads,” we used the analysis above as a
  starting point and followed it with primary research into fraud M.O.s. This led us to
  conclude that the “bad” labels could be grouped into two distinct FPF M.O.s, one
  related to check fraud and one related to ACH fraud. Those label sets were consistently
  associated with distinctive (though overlapping) subsets of the eight signals.
</p>

<p>
  We did not use statistical techniques like clustering on the tables above. We had on
  the order of 10 to 20 datasets and 10 to 20 signals (we included eight in the initial
  models), so we manually reviewed how each boolean signal performed on each dataset to
  decide what to include.
</p>

<p>
  Once we had decided on the two models and had a rough sense of which labels to include,
  we went back to the customers who provided the labels, shared our analysis and
  research, and, through engagement, refined our understanding of the label
  sub-categories. This helped us refine the labels further to align with the fraud M.O.s
  we were targeting.
</p>

<p>
  That refinement loop was so important for label quality that we ended up training each
  initial score on one customer’s data. Deep engagement on label sub-categories produced
  concentrated sets of 1,000 to 2,000 high-quality “bad” labels for each model. The
  “goods” were approved applications from the same customer over the same time period.
</p>

<h2>The scores did well</h2>

<p>Launched in early 2024, by the time I left SentiLink in late 2025:</p>

<ul>
  <li>
    Many top financial institutions were using them in fraud decisioning, including:
  </li>
  <li>Four top fifteen banks</li>
  <li>Two top ten credit card issuers</li>
  <li>A top five credit union</li>
  <li>
    One additional financial institution had signed to use them but had not yet
    integrated
  </li>
  <li>
    Nearly all of the top financial institutions SentiLink worked with were engaged in
    the retrostudy process with these scores
  </li>
</ul>

<p>
  These scores became SentiLink’s fastest-growing product launched during my last two
  years at the company, accounting for roughly 5% of ARR by the time I left, despite no
  marketing push. We typically offered existing customers a way to test these products
  alongside our synthetic fraud and identity theft scores, and more often than not they
  found the scores caught fraud our other products missed. In my final quarter, four of
  the top financial institutions mentioned above went live via API.
</p>

<p>
  We eventually created a single score combining the two models via a simple linear
  transformation. As I was transitioning out, the team was exploring training a unified
  model with labels from multiple partners. Still, the fact that the simple approach got
  us as far as it did holds several lessons, which we’ll cover next.
</p>

<h2>Lessons</h2>

<p>
  Evaluation affects and can illuminate everything. Just as how a SaaS company defines
  and breaks down ARR can affect and clarify how it operates, how you evaluate models can
  shape how you understand the problem and even how you define it. In our case, creating
  a more granular evaluation framework, breaking performance down into 10 to 15 datasets
  across a small number of key metrics, changed not just how we measured progress but how
  we understood the space of first party fraud itself.
</p>

<p>
  This framework was straightforward technically. We stored datasets in S3 and maintained
  a couple of version-controlled Python scripts that produced tables like the ones above,
  supported adding new datasets, and evaluated new features and scores against existing
  datasets.
</p>

<h2>What came next</h2>

<p>
  These scores helped initiate collaborations with major U.S. financial institutions
  around first party fraud that continue to this day. SentiLink has used these
  relationships to explore more determinative solutions for first party fraud, and a
  couple of the eight signals we developed later became useful building blocks for
  adjacent fraud efforts that were in progress when I left and that SentiLink may be
  announcing soon.
</p>

<p>
  More broadly, this work established practices at SentiLink for evaluating fraud signals
  on external datasets in a systematic way. As I was departing, the business launched an
  initiative to evaluate our identity theft score, and some of its key underlying
  signals, on external datasets using the same approach developed for FPF. We also built
  a similar evaluation framework for another fraud area and it helped us see that we
  likely could not produce compelling scores or flags there.
</p>

<h2>Evaluations beyond fraud models</h2>

<p>Discuss forthcoming post on evaluating quantized models.</p>

<h2>About the author</h2>

<p>
  Seth Weidman worked at SentiLink for about six years, from when the company was about
  two and a half years old (December 2019) to when it was eight and a half years old
  (November 2025).
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup><a href="#fnref1">1</a></sup>
  I won’t enumerate the specific fields here, but look at the application for a credit
  card or checking account with a major bank and you’ll get the idea.
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>

<p id="fn2" class="footnote">
  <sup><a href="#fnref2">2</a></sup>
  I won’t enumerate the specific fields here, but look at the application for a credit
  card or checking account with a major bank and you’ll get the idea.
  <a href="#fnref2" class="footnote-backref">↩</a>
</p>

<p id="fn3" class="footnote">
  <sup><a href="#fnref3">3</a></sup>
  Description of the “credit report” aspect of synthetic fraud and identity theft.
  <a href="#fnref3" class="footnote-backref">↩</a>
</p>

<p id="fn4" class="footnote">
  <sup><a href="#fnref4">4</a></sup>
  Assuming the overall fraud rate is 1%, flagging 30% of all fraud while flagging 2% of
  approvals implies roughly 15% precision, the precision value referenced in the main
  text.
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<br />
<button onclick="window.location.href = '/'">Back to Home</button>
