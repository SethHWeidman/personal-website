<link rel="stylesheet" href="/fraud-tables.css" />

<h1 class="post-title">
  Building Fraud Models with Heterogeneous Labels of Unknown Quality (draft)
</h1>
<p class="post-subtitle">
  How We Built a Model-Backed First Party Fraud Product at SentiLink
</p>
<p class="post-date">January 29, 2026</p>

<p>
  This essay describes how we successfully built a 0-to-1 fraud risk score product at
  SentiLink. We built this utilizing a machine learning model despite having to train on
  heterogeneous sets of labels where we couldn't directly verify quality or relevance.
  The problem was the broad category of "first party fraud" (FPF), a collection of fraud
  variants that our customers (FIs offering lending and depository products) typically
  discovered only "after the fact": they would observe unusually bad behavior from one of
  <em>their</em> customers and, possibly after doing additional manual review, conclude
  that first party fraud had occurred at the time of the application.
</p>

<p>
  The result was that our labels were a collection of customer-defined "cohorts"; in each
  one, a different "FPF-adjacent" behavior had been observed. Most were instances where
  our FI customers had observed behaviors highly suggestive of FPF, but some were cases
  where those customers had reached the FPF determination through manual review. Examples
  included sets of accounts where bad checks were passed, sets of accounts with large
  customer-initiated ACH returns, and cases where a customer determined that their
  customer had charged off and never had intent to pay (possibly because they planned to
  engage in
  <a href="https://www.transunion.com/faq/what-is-credit-washing">credit washing</a>
  later on).
</p>

<p>
  This presented three potential problems for building a score. The first was the
  heterogeneity of these labels: while we could have just thrown all these labels into a
  model, we ultimately had to sell these scores to conservative banks and credit unions
  that would want a coherent story for what our model was targeting, which this approach
  would be unlikely to provide. The second was mislabeling: some labels, especially those
  based mostly on our customers observing a bad outcome, may not have been due to FPF at
  all (the behaviors may have occurred due to identity theft, for example). Finally, in
  cases where a broad swath of applications was labeled "bad" during a fraud attack, some
  legitimate applications may have been swept up in the labeling.
</p>

<p>
  These properties were new for SentiLink. At the time we started working on these first
  party fraud scores in mid-2022, SentiLink had two successful products: scores to help
  FIs stop synthetic fraud and identity theft. SentiLink's two founders had built
  solutions to address these problems at Affirm, the online lender, and thus had strong
  priors on the building blocks needed to enable machine learning models tailored to
  solving them.
</p>

<p>
  We licensed data sources relevant to the patterns associated with synthetic fraud and
  identity theft and used these sources, along with data collected via our APIs, to build
  data structures relevant to detecting these fraud M.O.s. We built internal tooling to
  visualize these data structures and hired an internal team of fraud analysts (several
  of whom came from Affirm) to use this tooling to label subsets of applications our
  partners sent us.
</p>

<p>
  These curated labels, each based on manual review of on-prem data relevant to the
  application, were then used by the data science team to train our models. This set of
  approaches drove the company from a few months after founding (when the decision to
  focus on "scores" was made) through
  <a href="https://www.craftventures.com/articles/why-we-invested-in-sentilink"
    >the Series B fundraise from Craft Ventures in May 2021</a
  >.
</p>

<h2>Our FPF Problem and an Analogy to Understand It</h2>

<p>
  With first party fraud, we could not determine through manual review that an individual
  application was FPF in the same way that we could for synthetic fraud and ID theft. The
  main reason was that, as described above, FPF is typically discovered through unusually
  bad behavior on the part of an existing customer.
</p>

<p>
  Two other reasons relate to the definitions of the fraud types themselves. First,
  synthetic fraud and identity theft can often be detected due to "mismatches" in the PII
  on an application: a brand new social security number on a person in their 40s is a
  sign of first party synthetic fraud, and an address, phone number, and social security
  number all from different states is a sign of identity theft. Second, the data we had
  licensed and the data structures we built were highly tailored toward detecting
  synthetic fraud and identity theft.
</p>

<p>For example:</p>

<ul>
  <li>
    We had an "identity database," based mostly on licensed data, that was structured to
    "natively" show if someone was committing first party synthetic fraud (where one
    person uses multiple SSNs with their own name and date of birth).
  </li>
  <li>
    We had an "application graph" based mostly on first party data (e.g., our FI
    customers calling our API) that let us see if one phone number was being used on
    multiple applications with different pieces of core PII, a sign that it might be the
    phone of a fraudster who had stolen multiple identities.
  </li>
</ul>

<p>
  So we <em>had</em> to rely on labels from our partners, which had the issues of
  heterogeneity and accuracy described above.
</p>

<p>
  <strong>An analogy:</strong> suppose your task is to train a classifier to detect
  whether a car is in an image, given several "sets" of images handed to you by
  individuals of varying expertise. Maybe one person gave you two chunks (one of 1,000
  images and another of 2,000 images), a second person gave you a chunk of 3,000 images,
  and so on. However,
  <em
    >looking at the images yourself wouldn't actually confirm whether there was a car in
    the image!</em
  >
  While most chunks presumably do have cars in them, some may instead have motorcycles,
  some may have trucks, and some chunks may not even be images of vehicles at all.
</p>

<p>
  How to proceed? Suppose you can manually define certain "<strong>patterns</strong>" and
  check whether each pattern is present in each image. For example, you can detect
  whether there is a sheet of metal (which could be part of a door), whether there is a
  pane of glass (which would be a window or a windshield), whether there is a metal
  circle inside a rubber circle (which could indicate a wheel), and so on. You can also
  define patterns that might be present in an image containing a motorcycle or a truck
  (handlebars, a large cab), but not in one containing a car.
</p>

<p>
  This information, even taken all together, would not be sufficient to determine whether
  a set of images actually contains cars. However, from the prevalence of these patterns
  among the images, and from comparing this to the prevalence among a random set of
  images pulled from the internet, you can infer which chunks of images are
  <em>likely</em> to contain cars and thus are likely to be beneficial if included as
  "targets" in the "car" model.
</p>

<p>
  For us, the analogues to these patterns were signals we could observe in the data tied
  to these applications (in the "identity database," the "graph," and other on-prem data
  structures) that we hypothesized could be good signals for first party fraud.
</p>

<p>
  There was a virtually infinite universe of these potential signals we could engineer;
  we refined our possible set of hypotheses through:
</p>

<ul>
  <li>
    <strong>Primary research into first party fraud M.O.s.</strong> A common type of FPF
    involving ACH transactions requires multiple checking accounts, so we hypothesized
    that applying for multiple checking accounts in a short period might be predictive.
    We were able to observe this behavior in the application graph, even though it was
    built to identify identity theft.
  </li>
  <li>
    <strong>Reviewing cases from the ambiguous labels we'd received.</strong> Even though
    these labels were imperfect, we found certain anomalous patterns coming up
    repeatedly. For example, we could see in our identity database that a
    disproportionately high number of the supposed first party fraudsters had
    <em>previously</em> committed synthetic fraud, even if they were not committing
    synthetic fraud on the specific applications where they were labeled as first party
    fraud.
  </li>
</ul>

<p>
  While each of these plausibly could be a signal for first party fraud, we then had to
  validate them quantitatively using the heterogeneous, unreliable labels we had received
  from our customers. The next section describes how we did this.
</p>

<h2>Evaluating hypothesized signals on heterogeneous, unreliable labels</h2>

<p>Suppose we started with three sets of potential first party fraud labels:</p>

<ul>
  <li>
    One from a credit union, with 4,000 overall "first party fraud" labels divided into
    three subcategories.
  </li>
  <li>
    One from a large bank, consisting of a single chunk of 2,000 first party fraud
    labels.
  </li>
  <li>
    One from another large bank, with 18,000 first party fraud labels divided into two
    subcategories.
  </li>
</ul>

<p>
  For each dataset, we produced a table showing how each potential signal behaved on both
  the fraud labels and the relevant population of which the applications were a subset:
</p>

<div class="fraud-table-container">
  <h3>Credit Union 1</h3>
  <div class="fraud-table-scroll">
    <table id="credit-union-1-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>4,000</td>
          <td class="highlight">32.9%</td>
          <td class="highlight">39.0%</td>
          <td>2.6%</td>
          <td>3.0%</td>
          <td>4.4%</td>
          <td>1.3%</td>
          <td>1,317</td>
          <td>1,560</td>
          <td>105</td>
          <td>119</td>
          <td>177</td>
          <td>50</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>2,500</td>
          <td class="highlight">37.9%</td>
          <td class="highlight">30.0%</td>
          <td>3.3%</td>
          <td>1.4%</td>
          <td>1.3%</td>
          <td>1.2%</td>
          <td>948</td>
          <td>749</td>
          <td>82</td>
          <td>36</td>
          <td>33</td>
          <td>30</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>1,000</td>
          <td>3.8%</td>
          <td>3.8%</td>
          <td>2.8%</td>
          <td>1.7%</td>
          <td>2.7%</td>
          <td>2.8%</td>
          <td>38</td>
          <td>38</td>
          <td>28</td>
          <td>17</td>
          <td>27</td>
          <td>28</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_3</td>
          <td>500</td>
          <td class="highlight">32.8%</td>
          <td class="highlight">22.8%</td>
          <td>3.2%</td>
          <td>4.0%</td>
          <td>2.4%</td>
          <td>2.6%</td>
          <td>164</td>
          <td>114</td>
          <td>16</td>
          <td>20</td>
          <td>12</td>
          <td>13</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>200,000</td>
          <td>4.2%</td>
          <td>2.8%</td>
          <td>4.0%</td>
          <td>3.8%</td>
          <td>1.5%</td>
          <td>1.7%</td>
          <td>8,422</td>
          <td>5,676</td>
          <td>7,908</td>
          <td>7,581</td>
          <td>3,096</td>
          <td>3,368</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="fraud-table-container">
  <h3>Large Bank 1</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-1-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>2,000</td>
          <td>4.3%</td>
          <td class="highlight">15.1%</td>
          <td class="highlight">21.2%</td>
          <td class="highlight">27.4%</td>
          <td>1.2%</td>
          <td>3.2%</td>
          <td>86</td>
          <td>302</td>
          <td>425</td>
          <td>547</td>
          <td>24</td>
          <td>63</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>165,000</td>
          <td>4.4%</td>
          <td>2.1%</td>
          <td>2.5%</td>
          <td>3.6%</td>
          <td>4.3%</td>
          <td>3.4%</td>
          <td>7,179</td>
          <td>3,469</td>
          <td>4,192</td>
          <td>5,981</td>
          <td>7,076</td>
          <td>5,578</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="fraud-table-container">
  <h3>Large Bank 2</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-2-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />recall</th>
          <th>fpf_signal_2<br />recall</th>
          <th>fpf_signal_3<br />recall</th>
          <th>fpf_signal_4<br />recall</th>
          <th>high_synthetic_score_recall</th>
          <th>high_id_theft_score_recall</th>
          <th>n_pos_fpf_signal_1</th>
          <th>n_pos_fpf_signal_2</th>
          <th>n_pos_fpf_signal_3</th>
          <th>n_pos_fpf_signal_4</th>
          <th>n_high_synthetic_score</th>
          <th>n_high_id_theft_score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>18,000</td>
          <td>3.5%</td>
          <td>3.4%</td>
          <td>3.5%</td>
          <td>1.1%</td>
          <td>1.9%</td>
          <td>3.8%</td>
          <td>622</td>
          <td>606</td>
          <td>639</td>
          <td>200</td>
          <td>350</td>
          <td>683</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>12,500</td>
          <td>4.1%</td>
          <td>3.4%</td>
          <td>2.2%</td>
          <td>3.2%</td>
          <td>3.9%</td>
          <td class="highlight">17.8%</td>
          <td>513</td>
          <td>419</td>
          <td>269</td>
          <td>405</td>
          <td>482</td>
          <td>2,227</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>5,500</td>
          <td class="highlight">27.6%</td>
          <td class="highlight">30.3%</td>
          <td>4.5%</td>
          <td>2.0%</td>
          <td>2.9%</td>
          <td>4.0%</td>
          <td>1,517</td>
          <td>1,666</td>
          <td>248</td>
          <td>109</td>
          <td>160</td>
          <td>218</td>
        </tr>
        <tr>
          <td>overall_apps</td>
          <td>240,000</td>
          <td>2.6%</td>
          <td>8.2%</td>
          <td>8.9%</td>
          <td>8.4%</td>
          <td>2.6%</td>
          <td>5.8%</td>
          <td>2,580</td>
          <td>8,239</td>
          <td>8,947</td>
          <td>8,474</td>
          <td>2,753</td>
          <td>5,818</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<h2>What these tables tell us</h2>

<p>
  These tables show, for each individual chunk of fraud labels, the percent of those
  labels "flagged" by each individual signal (i.e., the "recall" of that signal on that
  chunk). For comparison, we also show in the bottom row of each table the percent of the
  overall dataset that was flagged by the signal.
</p>

<p>
  The ratio of these numbers was an important quantity for us. We called it "relative
  likelihood":
</p>

<pre><code>relative_likelihood =
  P(signal = 1 | label = fraud) / P(signal = 1)</code></pre>

<p>
  This is a proxy for precision that we would both look at internally and present along
  with recall. We found that it was easier for potential customers to translate "This
  feature flags 30% of your fraud while flagging 2% of your approved applications, a 15x
  ratio" into business value than "This feature has a 30% recall and a 15% precision."
  <sup id="fnref1"><a href="#fn1">1</a></sup>
</p>

<div class="fraud-table-container">
  <h3>Credit Union 1 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="credit-union-1-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>4,000</td>
          <td class="highlight">7.83</td>
          <td class="highlight">13.93</td>
          <td>0.65</td>
          <td>0.79</td>
          <td>2.93</td>
          <td>0.76</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>2,500</td>
          <td class="highlight">9.02</td>
          <td class="highlight">10.71</td>
          <td>0.82</td>
          <td>0.37</td>
          <td>0.87</td>
          <td>0.71</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>1,000</td>
          <td>0.90</td>
          <td>1.36</td>
          <td>0.70</td>
          <td>0.45</td>
          <td>1.80</td>
          <td>1.65</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_3</td>
          <td>500</td>
          <td class="highlight">7.81</td>
          <td class="highlight">8.14</td>
          <td>0.80</td>
          <td>1.05</td>
          <td>1.60</td>
          <td>1.53</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>200,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="fraud-table-container">
  <h3>Large Bank 1 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-1-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>2,000</td>
          <td>0.98</td>
          <td class="highlight">7.19</td>
          <td class="highlight">8.48</td>
          <td class="highlight">7.61</td>
          <td>0.28</td>
          <td>0.94</td>
        </tr>
        <tr>
          <td>overall_approvals</td>
          <td>165,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="fraud-table-container">
  <h3>Large Bank 2 (relative likelihood)</h3>
  <div class="fraud-table-scroll">
    <table id="large-bank-2-relative-table" class="fraud-table">
      <thead>
        <tr>
          <th>label category</th>
          <th>n_total</th>
          <th>fpf_signal_1<br />relative likelihood</th>
          <th>fpf_signal_2<br />relative likelihood</th>
          <th>fpf_signal_3<br />relative likelihood</th>
          <th>fpf_signal_4<br />relative likelihood</th>
          <th>high_synthetic_score<br />relative likelihood</th>
          <th>high_id_theft_score<br />relative likelihood</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bad_labels_overall</td>
          <td>18,000</td>
          <td>1.35</td>
          <td>0.41</td>
          <td>0.39</td>
          <td>0.13</td>
          <td>0.73</td>
          <td>0.66</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_1</td>
          <td>12,500</td>
          <td>1.58</td>
          <td>0.41</td>
          <td>0.25</td>
          <td>0.38</td>
          <td>1.50</td>
          <td class="highlight">3.07</td>
        </tr>
        <tr>
          <td>bad_label_sub_category_2</td>
          <td>5,500</td>
          <td class="highlight">10.62</td>
          <td class="highlight">3.70</td>
          <td>0.51</td>
          <td>0.24</td>
          <td>1.12</td>
          <td>0.69</td>
        </tr>
        <tr>
          <td>overall_apps</td>
          <td>240,000</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p>
  You'll note that we also included in these tables the recalls and relative likelihoods
  for scoring high on our synthetic fraud and identity theft models. Extending the cars,
  trucks, and bicycles analogy above: in addition to telling you whether there were
  certain material patterns present, imagine you also had models that you knew were
  80%–90% accurate at telling you whether the image contained a truck or a bicycle.
</p>

<h3>Making sense of these tables</h3>

<p>Looking at these six tables, several things jump out:</p>

<ul>
  <li>
    Signals 1 and 2 tend to "pop" on sub-categories 1 and 3 from the credit union, and
    sub-category 2 from the second large bank. These signals often "fire together," and
    they tend to fire on distinct sets of fraud-label datasets.
  </li>
  <li>
    Signal 1 does not meaningfully "pop" on Large Bank 1 overall (relative likelihood
    near 1), whereas Signals 2–4 do.
  </li>
  <li>
    Large Bank 2's sub-category 1 has a high rate of ID theft (measured by the percentage
    of these applications that score highly on our ID theft score); this indicated to us
    that this label set was likely mostly identity theft rather than first party fraud.
  </li>
  <li>
    Sub-category 2 from the credit union did not fire on any of our hypothesized FPF
    signals, suggesting either a different underlying M.O. or substantial label noise.
  </li>
</ul>

<h2>From signals to scores</h2>

<p>
  This analysis certainly helped us understand our data deeply, but how did we go from
  running these numbers to producing first party fraud scores?
</p>

<p>
  When modeling, before making technical decisions like which model structure to use
  (logistic regression or gradient-boosted trees) and how to select hyperparameters,
  there are decisions closer to business strategy that end up having a much bigger impact
  on the efficacy of the model you build:
</p>

<ul>
  <li>
    Which features (or "signals" as we tended to refer to them) to use, i.e., what the
    "columns" of the dataset behind the model will be.
  </li>
  <li>
    Which dataset or datasets you'll train on; this includes deciding both what the
    "bads" and the "not bads" should be.
  </li>
</ul>

<p>
  <strong>
    The analysis described above directly informs both of these questions. Without
    breaking down how key individual signals are (or are not) associated with various
    potential targets, you are largely taking shots in the dark.
  </strong>
  (An alternative approach is to feed every conceivable signal you can think of into the
  model. We learned some hard lessons about doing this with our synthetic and ID theft
  models: while it did not necessarily degrade aggregate model performance, it led to
  less explainable SHAP values and occasional misses that sometimes raised uncomfortable
  questions among our largest and most conservative customers.)
</p>

<p>
  Of the 10–20 potential signals we evaluated, we ended up finding eight that were
  conceptually distinct from one another and individually predictive on many of the
  datasets we'd received. By this, we mean that they were prevalent among many of the FPF
  label datasets we'd received and not so prevalent overall (in other words, high recall
  and high relative likelihood, respectively). Each of these eight signals was a boolean
  true/false flag, accompanied by numeric "sub-signals" (e.g., the number of distinct
  SSNs someone had committed synthetic fraud with, or the number of DDAs applied for in
  different time windows). These signals ultimately became ~40 features in our model.
</p>

<p>
  To choose which datasets to use as our "goods" and "bads," we used the analysis above
  as a starting point and followed it up with primary research into the fraud M.O.s. This
  led us to conclude that the "bad" labels we'd received could be broadly grouped into
  two distinct FPF M.O.s (one related to check fraud and one related to ACH fraud). We
  saw these bad labels consistently associated with distinctive, though overlapping, sets
  of the eight signals we'd found.
</p>

<p>
  Note that we did not use statistical techniques such as clustering on the tables above.
  We had on the order of 10–20 datasets and 10–20 signals (of which we ended up including
  eight in the initial models), so we simply manually reviewed the performance of each
  boolean signal on each dataset to determine what to include.
</p>

<p>
  Once we had decided on these two distinct models and had a general sense of the labels
  we wanted to include in each, we were able to go back to the customers from whom we'd
  received these labels, share our analysis and research, and dive deeper into these
  labels. Deep engagement allowed us to refine the labels we had down to fit the fraud
  M.O.s that we were targeting.
</p>

<p>
  This ability to refine the labels with input from the customers we received them from
  was so critical in increasing label quality that we ended up training each of the
  initial scores on one customer's data. Our collaboration with these customers about the
  sub-categories of labels they had sent led to concentrated sets of 1,000–2,000
  high-quality labels as the "bads" for each model; the "goods" were approved
  applications from the same time period, from those same customers.
</p>

<h2>The scores did well</h2>

<p>
  Launched in early 2024, by the time I left SentiLink in late 2025, many top FIs were
  using them as part of their fraud decisioning, including:
</p>

<ul>
  <li>Four top fifteen banks</li>
  <li>Two top ten credit card issuers</li>
  <li>A top five credit union</li>
</ul>

<p>
  One additional FI had signed to use them but had not yet integrated. Furthermore,
  nearly all of the top FIs SentiLink worked with were engaged in the retrostudy process
  with these scores.
</p>

<p>
  These scores became SentiLink's fastest-growing product launched during my last two
  years at the company, accounting for roughly 5% of ARR by the time I left despite no
  marketing push. We largely offered existing customers a way to test these products
  alongside our synthetic fraud and identity theft scores, and more often than not, they
  found that the scores caught fraud our other products missed. In my final quarter, four
  of the top FIs mentioned above went live via API.
</p>

<p>
  We eventually created a single score that combined the two underlying models via a
  simple linear transformation. As I was transitioning out, the team was exploring
  training a unified model that included labels from multiple partners. Still, the fact
  that the simple approach we took got us as far as it did holds several lessons.
</p>

<h2>Lessons</h2>

<p>
  Evaluation illuminates everything. Just as, at a macro level, how a SaaS company
  defines and breaks down its ARR can affect and elucidate how it is operating, deciding
  how to evaluate your models can affect everything. In particular, creating a more
  granular evaluation framework, as we did here (breaking down performance into 10–15
  individual datasets along a couple of key metrics) affected not just how we understood
  how well we were solving the problem of first party fraud, but even how we defined the
  problem itself. This framework was nothing fancy from a technical perspective: we
  stored the datasets themselves in S3, with a couple of version-controlled Python
  scripts that could produce tables like the ones above, add new datasets as needed, and
  evaluate new features and scores on our existing datasets.
</p>

<h2>What came next</h2>

<p>
  These scores allowed us to initiate collaborations with major U.S. FIs around first
  party fraud that continue to this day. SentiLink has used these relationships to
  explore more "determinative" solutions for first party fraud, and a couple of the eight
  signals we developed later became useful building blocks for adjacent fraud efforts
  that were in progress when I left and that SentiLink may be announcing soon.
</p>

<p>
  Most importantly, this work established at SentiLink a set of practices for evaluating
  fraud signals on external datasets in a systematic way. As I was departing, the
  business had launched an initiative to evaluate our identity theft score and some of
  its key underlying signals on external datasets, using the same approach we'd developed
  for FPF. We even built a similar evaluation framework for a new fraud area, and it
  helped us see that we likely couldn't produce compelling scores or flags there.
</p>

<h2>Evaluations beyond fraud models</h2>

<p>
  In a future post, I'll discuss how this framework applies to evaluating quantized
  models.
</p>

<h2>About the author</h2>

<p>
  Seth Weidman worked at SentiLink for about six years, from when the company was about
  two-and-a-half years old (December 2019) to when it was eight-and-a-half years old
  (November 2025).
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup><a href="#fnref1">1</a></sup>
  Assuming the overall fraud rate is 1%, flagging 30% of all fraud while flagging 2% of
  approvals implies roughly 15% precision (the precision value referenced in the main
  text).
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>

<br />
<button onclick="window.location.href = '/'">Back to Home</button>
