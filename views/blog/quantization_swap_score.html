<h1 class="post-title">
  A "Swap Score" For Evaluating Models When Stability Relative to a Baseline is Important
</h1>
<p class="post-subtitle">
  Why quantization benchmarks should report this swap score in addition to accuracy
  change
</p>
<p class="post-date">February 1, 2026</p>

<figure class="post-banner">
  <img
    src="quantization_swap_banner_placeholder.png"
    alt="Diagram showing model disagreement sets between V1 and V2."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  Single numbers, or even sets of numbers, about a dataset or a model's performance
  rarely tell the whole story. Anscombe's quartet is a classic example of this:
  <a
    href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet"
    target="_blank"
    rel="noopener noreferrer"
    >Anscombe's Quartet</a
  >; these four datasets have the same means of X and Y, and the same correlation
  coefficients, while producing vastly different visuals. At times the AI industry seems
  not to to have internalized this, relying too heavily on single numbers to characterize
  a single system's performance or a comparison between two systems. One scenario where
  this is apparent is model quantization; here, we care not only about the absolute
  performance of the model with quantized weights, but also its stability relative to the
  baseline model. For this situation, we propose a metric, the Weidman Swap Score, to
  quantify this behavior deviation; this Swap Score is relevant in any situation where
  stability of a modified model relative to a baseline is important.
</p>

<h2>Motivation</h2>

<p>
  A concrete example to motivate this: a chart in a recent NVIDIA blog post-I pick on
  them only because I read them voraciously and consider them the single best source of
  information on where the industry is going-inspired the chain of thought that led to
  this post. They illustrate the performance of their NVFP4 number format by showing that
  (along with other benchmarks) quantizing to it decreases accuracy on
  <a
    href="https://github.com/TIGER-AI-Lab/MMLU-Pro"
    target="_blank"
    rel="noopener noreferrer"
    >MMLU-Pro</a
  >
  from 85% to 84%. That single aggregate number can hide two very different
  under-the-hood realities, which could have significant implications for stability and
  reliability in production. Suppose this 85% to 84% drop was on a 100-question dataset,
  to keep the numbers simple. Either of the following two underlying realities could lead
  to this (for concision, we'll call the baseline model V1 and the quantized model V2):
</p>

<h3>Reality #1: "minimal swapping"</h3>

<ul>
  <li>
    All 15 questions V1 got wrong, V2 also got wrong. V2 simply gets
    <em>one additional</em> question wrong.
    <ul>
      <li>
        V1 and V2 disagree on precisely <strong>1</strong> question out of 100. Given the
        decrease in overall accuracy from 85% to 84%, this is the minimum possible number
        of disagreements.
      </li>
    </ul>
  </li>

  <h3>Reality #2: "maximal swapping"</h3>

  <li>
    The 15 questions that V1 got wrong, V2 gets
    <em>right</em>. However, V2 gets 16 <em>new</em> questions wrong that V1 got right.
    <ul>
      <li>V1 and V2 disagree on 15 + 16 = <strong>31</strong> questions out of 100.</li>
    </ul>
  </li>
</ul>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-02-02_quantization_swap_score/01_swap_scenarios.png"
    alt="Confusion matrices for two scenarios with an identical accuracy decrease"
  />
</figure>

<p>
  Despite one scenario involving disagreement on <em>1%</em> of the dataset and the other
  involving disagreement on <em>31%</em> of the dataset, in both cases we observe the
  same headline accuracy change: 85% to 84%. This is an Anscombe’s quartet-style
  situation: same top-level numbers, very different underlying reality.
</p>

<p>
  To be clear, I'm not claiming that NVIDIA is covering anything up by not reporting some
  kind of swap score: it’s possible that, compared to other formats that quantize models
  from 8 to 4 bits, NVFP4 induces
  <em>fewer</em> swaps. They are simply following the industry standard by only reporting
  top-level accuracy numbers. In the next section, I formalize “disagreement,” derive its
  bounds for a pair of accuracies, and then define a normalized score that can be
  reported alongside accuracy.
</p>

<h2>A Swap-Based Stability Metric</h2>

<p>
  In this section I propose a metric of how stable one model is relative to a baseline;
  in the AI context, I envision this being applied to <em>quantized</em> or
  <em>distilled</em> models, but there may be other contexts where it is relevant. For
  simplicity, refer to the baseline model as "V1" and the quantized model as "V2". Let
  the dataset contain \(N\) questions. For two model versions:
</p>

<ul>
  <li>\(M_1\) = number of questions V1 got right</li>
  <li>\(M_2\) = number of questions V2 got right</li>
  <li>\(N\) = total number of questions</li>
</ul>

<p>We can calculate the range of possible disagreements as follows:</p>

<p>\[ \text{Minimum_Disagreement} = |M_2 - M_1| \]</p>

<p>\[ \text{Maximum_Disagreement} = \min((N - M_2) + (N - M_1), N) \]</p>

<blockquote>
  The intuition for the “minimum with \(N\)" in the maximum disagreement formula is: if
  \(M_1\) and \(M_2\) are both less than 50%, it is possible for the two versions to
  disagree on <em>every</em> question. For example, if V1 gets 36% of the questions
  right, and V2 gets 32% right, it is possible that V2 gets every question wrong that V1
  gets right: the 32% that V2 got right would simply have to be a strict subset of the
  64% of questions V1 got right, which is certainly possible.
</blockquote>

<h2>The Weidman Swap Score (WSS)</h2>

<p>
  We can normalize the actual observed disagreement between these bounds to create a
  metric: the <strong>Weidman Swap Score</strong>. This is a value from 0 to 1 that
  indicates where the actual disagreement falls relative to the minimum and maximum
  possible disagreement.
</p>

<p>
  \[ WSS = \frac{\text{Actual_Disagreement} -
  \text{Min_Disagreement}}{\text{Max_Disagreement} - \text{Min_Disagreement}} \]
</p>

<h2>WSS Relevance</h2>

<p>
  WSS is most relevant when <strong>stability relative to a baseline</strong> matters. If
  a new architecture results in a model having 80% accuracy, over the prior version's 75%
  accuracy, you may care less about its stability relative to that prior version. WSS
  could still be informative-a high WSS might indicate the newer model has mastered
  different aspects of whatever subject the benchmark was measuring than the older
  model-but this may matter less. By contrast, in comparisons where stability
  <em>is</em> important, I expect WSS to be an important sanity check: if accuracy drops
  from 85% to 84%, there may be nothing to worry about; but if WSS is close to 1, that
  could imply that the modified model is unstable to an extent greater than the small
  accuracy drop suggests; the engineer may choose to experiment with different
  quantization or distillation methods before shipping.
</p>

<h2>Conclusion</h2>

<p>
  For a given accuracy decrease of a model relative to a baseline-due to quantization or
  distillation, for example-a lower WSS is preferable, suggesting that users of the model
  will experience more stability and need to do less work to modify their workflows that
  depend on the model's behavior. As people use LLMs for more and more aspects of their
  personal and professional lives, and as more and more agentic applications are built
  which have one or multiple LLMs at their core, I expect model stability between model
  versions (though I expect absolute performance to remain paramount); thus, researchers
  and companies that employ them should get in the habit of using this score to check
  model stability and/or developing their own scores for doing this. We may even see
  scenarios where companies decide users would prefer a model with a slightly larger drop
  in accuracy with a much lower WSS; I leave that judgment call to the researchers,
  product managers, and executives in charge of those models' performance.
</p>

<p>
  I look forward to seeing what reporting of WSS uncovers about how stable or unstable
  existing quantization and distillation methods are, and I hope it enables the industry
  to make more informed tradeoffs between absolute performance and consistency over time
  going forward.
</p>

<h2>Appendix 1: Numeric Examples</h2>

<h3>Example 1</h3>

<p>Let’s return to the example. Suppose that upon inspection, we find:</p>

<ul>
  <li>V2 got 3 questions wrong that V1 got right.</li>
  <li>V2 also got one additional question wrong (the net accuracy loss).</li>
  <li>However, V1 got 3 questions wrong that V2 got right (swaps).</li>
</ul>

<p>
  This implies the two versions disagree on a total of 7 questions. Using our variables
  for 85% and 84%:
</p>

<ul>
  <li>The minimum disagreement, \(min\_dis\), is 1.</li>
  <li>The maximum disagreement, \(max\_dis\), is 31.</li>
</ul>

<p>The Weidman Swap Score (WSS) is:</p>

<p>\[ WSS = \frac{7 - 1}{31 - 1} = 0.2 \]</p>

<h3>Example 2</h3>

<p>
  Let's look at a lower accuracy scenario. V1 gets 36 out of 100 right, whereas V2 gets
  only 32 right. Suppose V2 gets 18 of the 36 questions that V1 got right, but misses the
  other 18. Additionally, V2 gets 12 questions right that V1 missed. This implies that
  the two versions disagree on <strong>30</strong> questions out of 100.
</p>

<p>For the values of 36% and 32%:</p>

<ul>
  <li>The minimum disagreement, \(min\_dis\), is 4.</li>
  <li>The maximum disagreement, \(max\_dis\), is 100.</li>
</ul>

<p>The Weidman Swap Score is:</p>

<p>\[ WSS = \frac{30 - 4}{100 - 4} = 0.271 \]</p>

<h2>Appendix 2: Pseudocode</h2>

<pre><code class="language-python">def swap_metrics(correct_v1, correct_v2):
    assert len(correct_v1) == len(correct_v2)
    N = len(correct_v1)

    N1 = sum(correct_v1)
    N2 = sum(correct_v2)

    swap_outs = sum(c1 and (not c2) for c1, c2 in zip(correct_v1, correct_v2))
    swap_ins  = sum((not c1) and c2 for c1, c2 in zip(correct_v1, correct_v2))
    dis = swap_outs + swap_ins

    min_dis = abs(N2 - N1)
    max_dis = min(N1 + N2, (N - N1) + (N - N2))

    # Handle the (rare) degenerate case where min_dis == max_dis.
    if max_dis == min_dis:
        wss = 0.0
    else:
        wss = 100.0 * (dis - min_dis) / (max_dis - min_dis)

    return {
        "N": N,
        "N1": N1,
        "N2": N2,
        "swap_outs": swap_outs,
        "swap_ins": swap_ins,
        "disagreement": dis,
        "min_dis": min_dis,
        "max_dis": max_dis,
        "wss": wss,
    }</code></pre>

<br />
<button onclick="window.location.href = '/'">Back to Home</button>
