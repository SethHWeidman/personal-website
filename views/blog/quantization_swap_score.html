<h1 class="post-title">
  A "Swap Score" For Evaluating Models When Stability Relative to a Baseline is Important
</h1>
<p class="post-subtitle">
  Why quantization benchmarks should report this swap score in addition to accuracy
  change
</p>
<p class="post-date">February 1, 2026</p>

<figure class="post-banner">
  <img
    src="quantization_swap_banner_placeholder.png"
    alt="Diagram showing model disagreement sets between V1 and V2."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  Single numbers, or even sets of numbers, about a dataset or a model's performance
  rarely tell the whole story. Anscombe's quartet is a classic example of this:
  <a
    href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet"
    target="_blank"
    rel="noopener noreferrer"
    >Anscombe's Quartet</a
  >; these four datasets have the same means of X and Y, and the same correlation
  coefficients, while producing vastly different visuals. At times the AI industry seems
  not to to have internalized this, relying too heavily on single numbers to characterize
  a single system's performance or a comparison between two systems. One scenario where
  this is apparent is model quantization; here, we care not only about the absolute
  performance of the model with quantized weights, but also its stability relative to the
  baseline model. For this situation, we propose a metric, the Weidman Swap Score, to
  quantify this behavior deviation; this Swap Score is relevant in any situation where
  stability of a modified model relative to a baseline is important.
</p>

<h2>Motivation</h2>

<p>
  A concrete example to motivate this: a chart in a recent NVIDIA blog post-I pick on
  them only because I read them voraciously and consider them the single best source of
  information on where the industry is going-inspired the chain of thought that led to
  this post. They illustrate the performance of their NVFP4 number format by showing that
  (along with other benchmarks) quantizing to it decreases accuracy on
  <a
    href="https://github.com/TIGER-AI-Lab/MMLU-Pro"
    target="_blank"
    rel="noopener noreferrer"
    >MMLU-Pro</a
  >
  from 85% to 84%. That single aggregate number can hide two very different
  under-the-hood realities, which could have significant implications for stability and
  reliability in production. Suppose this 85% to 84% drop was on a 100-question dataset,
  to keep the numbers simple. Either of the following two underlying realities could lead
  to this (for concision, we'll call the baseline model V1 and the quantized model V2):
</p>

<h3>Reality #1: "minimal swapping"</h3>

<ul>
  <li>
    All 15 questions V1 got wrong, V2 also got wrong. V2 simply gets
    <em>one additional</em> question wrong.
    <ul>
      <li>
        V1 and V2 disagree on precisely <strong>1</strong> question out of 100. Given the
        decrease in overall accuracy from 85% to 84%, this is the minimum possible number
        of disagreements.
      </li>
    </ul>
  </li>

  <h3>Reality #2: "maximal swapping"</h3>

  <li>
    The 15 questions that V1 got wrong, V2 gets
    <em>right</em>. However, V2 gets 16 <em>new</em> questions wrong that V1 got right.
    <ul>
      <li>V1 and V2 disagree on 15 + 16 = <strong>31</strong> questions out of 100.</li>
    </ul>
  </li>
</ul>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-02-02_quantization_swap_score/01_swap_scenarios.png"
    alt="Confusion matrices for two scenarios with an identical accuracy decrease"
  />
</figure>

<p>
  Despite one scenario involving disagreement on <em>1%</em> of the dataset and the other
  involving disagreement on <em>31%</em> of the dataset, in both cases we observe the
  same headline accuracy change: 85% to 84%. This is an Anscombe’s quartet-style
  situation: same top-level numbers, very different underlying reality.
</p>

<p>
  To be clear, I'm not claiming that NVIDIA is covering anything up by not reporting some
  kind of swap score: it’s possible that, compared to other formats that quantize models
  from 8 to 4 bits, NVFP4 induces
  <em>fewer</em> swaps. They are simply following the industry standard by only reporting
  top-level accuracy numbers. In the next section, I formalize “disagreement,” derive its
  bounds for a pair of accuracies, and then define a normalized score that can be
  reported alongside accuracy.
</p>

<h2>A Swap-Based Stability Metric</h2>

<p>
  In this section I propose a metric of how stable one model is relative to a baseline;
  in the AI context, I envision this being applied to <em>quantized</em> or
  <em>distilled</em> models, but there may be other contexts where it is relevant. For
  simplicity, refer to the baseline model as "V1" and the quantized model as "V2". Let
  the dataset contain \(N\) questions. For two model versions:
</p>

<ul>
  <li>\(M_1\) = number of questions V1 got right</li>
  <li>\(M_2\) = number of questions V2 got right</li>
  <li>\(N\) = total number of questions</li>
</ul>

<p>We can calculate the range of possible disagreements as follows:</p>

<p>\[ \text{Minimum_Disagreement} = |M_2 - M_1| \]</p>

<p>\[ \text{Maximum_Disagreement} = \min((N - M_2) + (N - M_1), N) \]</p>

<blockquote>
  The intuition for the “minimum with \(N\)" in the maximum disagreement formula is: if
  \(M_1\) and \(M_2\) are both less than 50%, it is possible for the two versions to
  disagree on <em>every</em> question. For example, if V1 gets 36% of the questions
  right, and V2 gets 32% right, it is possible that V2 gets every question wrong that V1
  gets right: the 32% that V2 got right would simply have to be a strict subset of the
  64% of questions V1 got right, which is certainly possible.
</blockquote>

<h2>The Weidman Swap Score (WSS)</h2>

<p>
  We can normalize the observed disagreement between these bounds to create a single
  metric: the <strong>Weidman Swap Score</strong> (WSS). WSS is a percentage from 0 to
  100 indicating where the actual disagreement falls between the minimum and maximum
  possible disagreement for the observed accuracies.
</p>

<p>
  \[ WSS = 100 \cdot \frac{\text{Actual Disagreement} - \text{Min Disagreement}}
  {\text{Max Disagreement} - \text{Min Disagreement}} \]
</p>

<h2>Caveats</h2>

<p>
  WSS is most relevant when <strong>stability relative to a baseline</strong> matters. If
  a model improves from 75% to 80% on some benchmark, you may care less about how
  “stable” it is with respect to the prior version—though WSS can still be informative. A
  high WSS in that setting might indicate the newer model has learned different aspects
  of the benchmark than the older model.
</p>

<p>
  I expect WSS to be most useful as a sanity check during quantization. If accuracy drops
  from 85% to 84%, there may be nothing to worry about; but if WSS is close to 100, that
  implies the quantized model disagrees with the baseline nearly as much as it possibly
  can (given those accuracies). In that case, you might try a less volatile quantization
  method before shipping.
</p>

<h3>Example 1: The 85% to 84% Drop</h3>

<p>Suppose we measure swap behavior and find:</p>

<ul>
  <li>V2 got <strong>4</strong> questions wrong that V1 got right (swap-outs).</li>
  <li>V2 got <strong>3</strong> questions right that V1 got wrong (swap-ins).</li>
</ul>

<p>
  Then the versions disagree on \(4 + 3 = 7\) questions. For \(M_1 = 85\), \(M_2 = 84\),
  \(N = 100\):
</p>

<ul>
  <li>\(\text{Min Disagreement} = |84 - 85| = 1\)</li>
  <li>
    \(\text{Max Disagreement} = \min(85 + 84,\; 200 - 169) = \min(169,\; 31) = 31\)
  </li>
</ul>

<p>\[ WSS = 100 \cdot \frac{7 - 1}{31 - 1} = 20 \]</p>

<h3>Example 2: Lower-Accuracy Models (36% and 32%)</h3>

<p>
  Now consider a lower-accuracy scenario: V1 gets 36% right and V2 gets 32% right on a
  100-question benchmark.
</p>

<p>
  Suppose V2 gets 19 of the 36 questions that V1 got right, but misses the other 17.
  Additionally, V2 gets 13 questions right that V1 missed. Then:
</p>

<ul>
  <li>swap-outs \(= 17\)</li>
  <li>swap-ins \(= 13\)</li>
  <li>\(\text{Actual Disagreement} = 30\)</li>
</ul>

<p>For \(M_1 = 36\), \(M_2 = 32\), \(N = 100\):</p>

<ul>
  <li>\(\text{Min Disagreement} = |32 - 36| = 4\)</li>
  <li>\(\text{Max Disagreement} = \min(36 + 32,\; 200 - 68) = \min(68,\; 132) = 68\)</li>
</ul>

<p>\[ WSS = 100 \cdot \frac{30 - 4}{68 - 4} \approx 40.6 \]</p>

<h2>Conclusion</h2>

<p>
  For a fixed accuracy decrease due to quantization, a
  <strong>lower WSS is preferable</strong>. It indicates the quantized model is
  preserving the baseline model’s specific knowledge, rather than “forgetting” old
  answers and “learning” new ones in a way that looks like volatility or noise.
</p>

<p>
  There may even be scenarios where a slightly larger drop in accuracy that corresponds
  to a much lower WSS is a better tradeoff; that judgment depends on the application.
  More broadly, researchers (and the labs and companies that employ them) should report
  swap-ins and swap-outs—or at least WSS—alongside accuracy when publishing quantization
  benchmarks.
</p>

<br />
<button onclick="window.location.href = '/'">Back to Home</button>
