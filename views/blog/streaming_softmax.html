<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Streaming Softmax: A Trick Powering FlashAttention</title>

    <script src="/mathjax-config.js"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
    ></script>

    <!-- Open Graph tags (for LinkedIn, etc.) -->
    <meta
      property="og:title"
      content="Streaming Softmax: A Trick Powering FlashAttention"
    />
    <meta
      property="og:description"
      content="A beautiful trick that lets us compute softmax-related vector functions in a 'streaming' fashion."
    />
    <meta
      property="og:url"
      content="https://www.sethweidman.com/blog/streaming_softmax.html"
    />
    <meta
      property="og:image"
      content="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/banner-image.png"
    />
    <meta property="og:type" content="article" />

    <!-- Twitter Card tags (for X) -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta
      name="twitter:title"
      content="Streaming Softmax: A Trick Powering FlashAttention"
    />
    <meta
      name="twitter:description"
      content="A beautiful trick that lets us compute softmax-related vector functions in a 'streaming' fashion."
    />
    <meta
      name="twitter:image"
      content="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/banner-image.png"
    />

    <!-- highlight.js CSS + JS -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/highlight.js@11/styles/default.min.css"
    />
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11/lib/highlight.min.js"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        hljs.highlightAll();
      });
    </script>
  </head>
  <body>
    <h1 class="post-title">Streaming Softmax</h1>
    <p class="post-subtitle">A Trick Powering FlashAttention</p>
    <p class="post-date">December 12, 2025</p>

    <figure class="post-banner">
      <img
        src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/banner-image.png"
        alt="Conceptual illustration of streaming softmax, showing data flowing through a process block by block."
        class="wide-image"
        loading="eager"
      />
    </figure>

    <p>
      Softmax is an essential operation in AI computing, taking in a vector and returning
      a normalized vector that sums to one. However, the normalization step means every
      element depends on all of the other elements in the vector. Nevertheless, in this
      essay, we'll cover a beautiful trick that lets vector functions closely related to
      softmax, such as:
    </p>

    <ul>
      <li>
        <strong>Sum of scaled exponentials</strong> (the numerically-stable softmax
        denominator)
      </li>
      <li><strong>Softmax dot</strong> (a scalar of the form softmax\((q)\) · \(v\))</li>
    </ul>

    <p>
      be computed in an "online" or "streaming" fashion, meaning that we process the of
      vectors one chunk at a time without loading the entire thing into memory. As it
      turns out, streaming the second of these, "softmax dot", is an essential component
      of the whole family of FlashAttention algorithms (a family of GPU kernels that
      compute attention using a "tiling" approach similar to the one described
      <a href="https://www.sethweidman.com/blog/cuda_matmul.html">here</a>). Instead of
      diving into "softmax dot", we'll start by showing how the core trick works in the
      even simpler setting of "the sum of scaled exponentials".
    </p>

    <h2>Background: The Numerical Stability Trick</h2>

    <p>As a refresher, the definition of softmax for a vector \(\mathbf{x}\) is:</p>

    <figure>
      <img
        src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/01-streaming-softmax-definition.jpg"
        alt="The mathematical formula for the softmax function, showing e to the power of x_j divided by the sum of e to the power of x_i for all i."
      />
    </figure>

    <p>
      All those exponentials could lead to the values in the softmax vector getting large
      enough to lead to numeric instability. Softmax has an extremely interesting
      property that allows us to ensure numerical stability:
    </p>

    <figure>
      <img
        src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/02-streaming-softmax-stability.jpg"
        alt="The mathematical formula showing that subtracting a constant 'c' from each element of the input vector to softmax does not change the result."
      />
    </figure>

    <p>
      Below is the math that shows why this is the case: for any given index \(j\) (1, 2,
      and so on up to \(n\)), the "jth element" of the vector
      \(\operatorname{softmax}(\mathbf{x} - c)\) is:
    </p>

    \[ \frac{e^{x_j - c}}{\sum_{i=1}^n e^{x_i - c}} = \frac{e^{-c} * e^{x_j}}{e^{-c} *
    \sum_{i=1}^n e^{x_i}} = \frac{e^{x_j}}{\sum_{i=1}^n e^{x_i}} \]

    <p>
      Taking advantage of this property, in AI computing, we almost always subtract the
      <strong>maximum value of the vector</strong> (\(M = \max(\mathbf{x})\)) from every
      element prior to the softmax operation. This ensures the largest element of
      softmax(\(\mathbf{x}\)) is \(e^{m-m} = e^0 = 1\), and avoids us having to compute,
      for example \(e^{22}\) which is about 3.5 billion or \(e^{222}\) which is almost
      one googol.
    </p>

    <h2 id="sum-of-scaled-exponentials">Streaming the "Sum of Scaled Exponentials"</h2>

    <p>
      So we want to compute the sum of the exponentials of the vector elements,
      <em
        >while subtracting the maximum of the vector and thus ensuring numeric
        stability</em
      >; that is, we want to compute:
    </p>

    \[ L = \sum_{i} e^{x_i - M} \]

    <p>Where \(M\) is the maximum of the <em>entire</em> vector \(\max(\mathbf{x})\).</p>

    <p>
      How can we do this in a “online” fashion? That is, only ever “loading” one chunk of
      \(\mathbf{x}\) at a time? Clearly, the difficult part is “keeping track of the
      maximum": theoretically we can't know what \(M\) is until we’ve seen the entire
      vector, but we still want to process each chunk as it comes in. We’ll now show
      "proof by induction" style, how to do this.
    </p>

    <p>
      Suppose we get the first block of, say, 50 elements. We can compute the maximum of
      this block; let's call it \(m_1\). We'll then "scale the exponentials by \(m_1\)"
      (we'll re-use this "scale by \(m_1\)" terminology later), meaning we first subtract
      \(m_1\) from each elements, take the exponentials, and sum the elements. So the sum
      of this block will just be \[ s_1 = \sum_{i=1}^{50} e^{x_i - m_1} = e^{x_1 - m_1} +
      e^{x_2 - m_1} + \cdots + e^{x_{50} - m_1}. \]
    </p>

    <p>
      Now suppose we have processed some number of blocks - say nine of them - and we
      have a running maximum \(M_9\) and a running sum \(S_9\), where the running sum is
      appropriately scaled by the maximum we have seen so far (clearly, thinking about
      the prior paragraph, after the first block, the elements will be correctly scaled
      by the maximum seen so far, \(m_1\)). What do we do upon receiving a
      <em>new</em>, tenth block of 50 elements?
    </p>

    <p>
      Let's recall what we have to do: running sum \(S_{10}\), scaled by whatever the
      maximum \(M_{10}\) is after seeing this block. To update the maximum, we simply
      take the maximum of what we've seen so far with the maximum of the new block:
    </p>

    \[ M_{10} = \max\left(M_9,\, m_{10}\right). \]

    <p>
      How do we update the running sum \(S_{10}\)? There are two cases to consider: the
      easy case, and the hard case where we'll have to use the elegant "Rescaling Trick"
      which is the core of this blog post.
    </p>

    <p>First, the easy case: \(m_{10}\) is smaller than \(M_9\). That is:</p>

    \[ M_{10} = \max\left(M_9,\, m_{10}\right) = M_9 \]

    <p>Then if the new block's elements are</p>

    \[ b_1, b_2, \ldots, b_{50} \],

    <p>we can simply add</p>

    \[ e^{b_1 - M_9} + e^{b_2 - M_9} + \cdots + e^{b_{50} - M_9} \]

    <p>
      to the running sum \(S_9\) to get \(S_{10}\). Once we update the new \(M_{10}\) to
      simply be \(M_9\), we'll be done processing this block.
    </p>

    <p>
      Now, the hard case: \(m_{10}\) is larger than \(M_9\) (so that \(M_{10}\), the
      running maximum after we update this block, is \(m_{10}\), the maxmimum of this
      block). In this case, we have \(S_{9}\), which has been scaled by a smaller number
      \(M_9\), and we want to adjust it to have a value
      <em>as if</em> it were actually scaled by what we now know to be the
      <em>true</em> "maximum seen so far", \(M_{10}\).
    </p>

    <h3>The Rescaling Trick</h3>

    <p>
      Remember what it means for an element to be "scaled by \(M_9\)": it just means that
      the element \(x_j\) has been transformed to \(e^{x_j - M_9}\). Look what happens
      when we multiply such an element by \(e^{M_9 - M_{10}}\):
    </p>

    \[ e^{x_j - M_9} \cdot e^{M_9 - M_{10}} = e^{x_j - M_9 + M_9 - M_{10}} = e^{x_j -
    M_{10}} \]

    <p>
      Thus, multiplying by \(e^{M_9 - M_{10}}\) transforms \(S_{9}\) into what its value
      would be <em>if it had been scaled by \(M_{10}\) all along</em>. It turns out that
      indeed, multiplying by \(e^{M_9 - M_{10}}\) "rescales" \(S_{9}\) in exactly the
      right way. Note why this makes sense intuitively: since \(M_9\) is smaller than
      \(M_{10}\), \(e^{M_9 - M_{10}}\) is a number between 0 and 1. In some sense, we
      were "too optimistic" with \(S_{9}\) previously, scaling it according to a maximum
      that was smaller than the <em>actual</em> maximum by which we should have been
      scaling it.
    </p>

    <h3>The Code</h3>

    <p>
      These two scripts on GitHub
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/03_streaming_softmax"
        target="_blank"
        rel="noopener noreferrer"
        >here</a
      >
      implement this. The
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/03_streaming_softmax/01_sum_of_exponentials_simple_example.py"
        target="_blank"
        rel="noopener noreferrer"
        >first script</a
      >
      shows the rescaling happening concretely when processing a vector in two chunks;
      the
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/03_streaming_softmax/02_sum_of_exponentials_large_example.py"
        target="_blank"
        rel="noopener noreferrer"
        >second script</a
      >
      shows that the code still works - that is, the sums when processing this vector the
      normal way and the streaming way are identical - when processing a vector of length
      1,000 filled with random integers between 0 and 100, processed in blocks of 50.
    </p>

    <p>
      The second vector function we cover here is slightly more complicated, and also
      more directly tied to attention - but it relies on the same straightforward trick
      in order to stream it!
    </p>

    <h2 id="softmax-dot">Streaming "Softmax Dot"</h2>

    <p>
      The second vector-to-scalar function I call \(\operatorname{softmax\text{-}dot}\):
      the "\(\operatorname{softmax\text{-}dot}\)" of two vectors \(\mathbf{q}\) and
      \(\mathbf{v}\) is:
    </p>

    <p>
      \[ \operatorname{softmax\text{-}dot}(\mathbf{q}, \mathbf{v}) =
      \operatorname{softmax}(\mathbf{q}) \cdot \mathbf{v} \]
    </p>

    <h3>Motivation: Softmax-Dot in Attention</h3>

    <p>
      One of the core steps in attention can be described in terms of this operation.
      Attention is often written in the following technically-correct way that
      nevertheless barely, if at all, elucidates the ideas involved:
    </p>

    \[ \operatorname{Attention}(Q, K, V) =
    \operatorname{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]

    <p>
      I bring this up just to make the following point about where softmax-dot is used in
      this:
    </p>

    <figure>
      <img
        src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-12_streaming-softmax/03-streaming-softmax-in-attention.jpg"
        alt="The attention formula with a red box highlighting the softmax of the scaled dot-product of Q and K-transpose, which is then multiplied by V."
      />
    </figure>

    <p>Now that we're motivated, let's actually discuss how to compute this thing.</p>

    <h3>Streaming "Softmax Dot" (for real this time)</h3>

    <p>
      So how can we actually stream this sum? First, note that each entry of
      \(\operatorname{softmax}(\mathbf{q})\) is the fraction
      \(\frac{e^{q_j}}{\sum_{i=1}^n e^{q_i}}\). Therefore, we can rewrite
      \(\operatorname{softmax\text{-}dot}(\mathbf{q}, \mathbf{v})\) as:
    </p>

    <p>
      \[ \operatorname{softmax\text{-}dot}(\mathbf{q}, \mathbf{v}) = \frac{\sum_{i=1}^n
      e^{q_i - M} v_i}{\sum_{i=1}^n e^{q_i - M}}, \qquad M = \max_j q_j. \]
    </p>

    <p>But we know how to stream both of these! We can calculate the denominator,</p>

    <p>\[ e^{q_1 - M} + e^{q_2 - M} + \cdots + e^{q_n - M}. \]</p>

    <p>
      in a streaming fashion using
      <em>exactly</em> the method we just covered in the prior section - it is literally
      a <strong>sum of scaled exponentials</strong>! And the numerator,
    </p>

    <p>\[ e^{q_1 - M} v_1 + e^{q_2 - M} v_2 + \cdots + e^{q_n - M} v_n. \]</p>

    <p>
      can be streamed using the same high level method as the sum of scaled exponentials
      - keeping track of the maximum of each block, and using the Rescaling Trick on the
      "accumulated sum" when we get a block with a greater maximum than we've seen so far
      - we just add \(e^{q_i - M} v_i\) elements to our running sum instead of \(e^{q_i -
      M}\) elements as before.
    </p>

    <p>
      The key point is that <em>the same rescaling trick works</em> for softmax-dot: the
      trickiest part of this computation is dealing with the fact that the maximum value
      of \(\mathbf{q}\) we've seen so far can change as we get more elements, and we
      handle both checking for that and rescaling using \(e^{m_{old} - m_{new}}\) in
      exactly the same way!
    </p>

    <p>
      As with the first two scripts, the
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/03_streaming_softmax/03_softmax_dot_product_streaming_example.py"
        target="_blank"
        rel="noopener noreferrer"
        >third script</a
      >
      in the GitHub repo shows that for two length 100 vectors of random floats between 0
      and 2 (arbitrarily), computing the softmax-dot the normal way and the streaming way
      yields the same result.
    </p>

    <h2>Conclusion</h2>

    <p>
      The block-by-block computation of the softmax-dot operation, along with the
      <a href="https://www.sethweidman.com/blog/cuda_matmul.html">tiling approach</a>
      discussed in my previous post is the secret sauce behind the massive speedups seen
      in FlashAttention. In a future post and/or implementation, we'll put these ideas
      together to show how a fuller version of FlashAttenion works.
    </p>
  </body>
</html>
