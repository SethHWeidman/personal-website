<h1 class="post-title">FlashAttention - Pt. 1</h1>
<p class="post-subtitle">How Tiling and Streaming Softmax Enable this GPU Kernel</p>
<p class="post-date">January 12, 2026</p>

<figure class="post-banner">
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/02_flash_attention_from_attention.png"
    alt="FlashAttention as a replacement for many of the multihead attention steps."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  This blog post will explain the FlashAttention algorithm, showing how it builds upon
  the concepts from two prior blog posts:
  <a href="/blog/cuda_matmul.html"><strong>tiling</strong></a
  >, and <a href="/blog/streaming_softmax.html"><strong>streaming softmax</strong></a
  >. We intentionally keep this post “whiteboard-level”; we'll link to an implementation
  at the end, but save the deep dive into it for a future blog post.
</p>

<h2>What is FlashAttention?</h2>

<p>
  FlashAttention is a drop-in replacement for most of the Multi-Head Attention operation,
  the foundational building block for modeling sequences introduced in
  <em>Attention is All You Need</em> and described in more detail (with a reference
  implementation) <a href="/blog/multihead_attention.html">here</a>. It was released in
  mid-2022 by a team led by
  <a href="https://tridao.me" target="_blank" rel="noopener noreferrer">Tri Dao</a>, then
  at Stanford. This release, now known as "FlashAttention v1", already provided a<a
    href="https://www.youtube.com/watch?v=gMOAud7hZg4&t=31m55s"
    target="_blank"
    rel="noopener noreferrer"
    >13% speedup</a
  >
  over highly-optimized CUDA kernels written <em>by NVIDIA</em> for their own GPUs! This
  work was the first to apply the concepts mentioned above - tiling and streaming softmax
  - to an attention kernel. Subsequent versions have employed much more sophisticated
  software features of GPUs, some of which only work on the latest and greatest
  "Blackwell" systems, and have had varying degrees of involvement from Prof.
  Dao[footnote], but the basic ideas introduced in v1 continue to be core.
</p>

<p>
  Before we dive in: FlashAttention is sometimes described as "a drop-in replacement for
  attention". That is not entirely accurate; it replaces <em>many</em> of the steps
  involved in Multi-Head Attention, but not all. Below is a reminder of all the steps
  involved in Multi-Head Attention:
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/01_multihead_attention_from_attention.png"
    alt="Standard Multi-Head Attention requires computing entire attention matrices within each head."
  />
</figure>

<p>
  FlashAttention fuses many of these operations; in particular, standard Multi-Head
  Attention requires "materializing" - defining and holding as one object in memory -
  entire "sequence_length by sequence_length" attention matrices within each head,
  whereas FlashAttention is able to compute the same final result while avoiding this
  memory-intensive materialization.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/02_flash_attention_from_attention.png"
    alt="FlashAttention fuses operations many operations, avoiding ever materializing an attention matrix."
  />
</figure>

<p>So how does it do it?</p>

<h2>FlashAttention v1</h2>

<p>
  We'll start by giving two high level mental models for what FlashAttention is doing
  that can be helpful to keep in mind when going through the specific steps of the
  algorithm:
</p>

<p>
  First: in this <a href="/blog/cuda_matmul.html"><strong>blog post</strong></a
  >, we describe a technique for computing the output of an operation involving
  <em>two</em> input matrices by dividing the matrices into "tiles", successively feeding
  those tiles into fast "shared" memory on the GPU, and using those tiles to compute
  partial sums which were then "accumulated" in the output we wanted to compute. The
  technique we'll use for FlashAttention is very similar (which is why reading through
  and making you understand that blog post is great scaffolding for understanding
  FlashAttention), except now we have <em>three</em> input matrices - Q, K, and V - and
  the operation in question is the attention operation:
</p>

\[ \operatorname{Attention}(Q, K, V) =
\operatorname{softmax}\!\left(\frac{QK^T}{\sqrt{d}}\right)V \]

<p>rather than simply a matrix multiplication.</p>

<p>
  Second, you might ask: how are we dealing with two dimensional Tensors at all in
  FlashAttention? Q, K, and V, after all, are typically <em>four</em>-dimensional Tensors
  of shape <code>[batch_size, sequence_length, head_dim, num_heads]</code> in attention
  implementations; even if you discount the batch dimension--operations are always the
  same within each batch element in neural networks, so that they can be thought of as
  "batch_size" identical operations happening in parallel--you are still left with three
  dimensional Tensors of shape: <code>[sequence_length, head_dim, num_heads]</code>. Here
  we take advantage of a further parallel structure in Multi-Head Attention: the
  computations <em>within each head</em> are identical. Thus, naively, in FlashAttention
  v1, we launch a GPU kernel with a grid of B * H thread blocks, each of which operates
  on a single [batch_index, head_index] tuple, and loads in tiles of the
  <code>[sequence_length, head_dim]</code> sections of Q, K, and V corresponding to that
  <code>batch_index</code> and <code>head_index</code> in order to compute a 2D
  <code>[sequence_length, head_dim]</code> section of the output. So, the algorithm we'll
  describe starting in the next section really is a "three 2D matrix" analogue of the
  "two 2D matrix" tiling algorithm described in the prior blog post; "the magic of CUDA"
  scales this up to be the 4D Tensor operation we need.
</p>

<p>With these preliminaries out of the way, let's dive into the algorithm.</p>
