<h1 class="post-title">FlashAttention: Algorithm and Pseudocode</h1>
<p class="post-subtitle">How Tiling and Streaming Softmax Enable this GPU Kernel</p>
<p class="post-date">January 27, 2026</p>

<figure class="post-banner">
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/flash_attention_banner.png"
    alt="FlashAttention computes O by tiling Q, K, and V."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  This blog post will explain the FlashAttention algorithm, showing how it builds upon
  the concepts from two prior blog posts:
  <a href="/blog/cuda_matmul.html"><strong>tiling</strong></a
  >, and <a href="/blog/streaming_softmax.html"><strong>streaming softmax</strong></a
  >. More specifically, this post will focus on a "whiteboard-level" understanding of the
  algorithm, and, where helpful, will link out to a
  <a
    href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py"
    target="_blank"
    rel="noopener noreferrer"
    >Python implementation</a
  >
  that mimics how the algorithm would be coded in CUDA; we'll save a full CUDA
  walkthrough for a future blog post (though readers may find it a good exercise to
  implement FlashAttention in CUDA after reading this blog post, using the Python
  implementation as a starting point).
</p>

<h2>What is FlashAttention? Background and historical context</h2>

<p>
  FlashAttention is a drop-in replacement for many of the steps of the Multi-Head
  Attention operation, which itself is the foundational building block for modeling
  sequences introduced in <em>Attention is All You Need</em> and is described in more
  detail (with a couple reference implementations) in a prior blog post
  <a href="/blog/multihead_attention.html">here</a>. It was released in mid-2022 by a
  team led by
  <a href="https://tridao.me" target="_blank" rel="noopener noreferrer">Tri Dao</a>, then
  at Stanford. This release, now known as "FlashAttention V1", already provided a<a
    href="https://www.youtube.com/watch?v=gMOAud7hZg4&t=31m55s"
    target="_blank"
    rel="noopener noreferrer"
    >13% speedup</a
  >
  over highly-optimized CUDA kernels written <em>by NVIDIA</em> for their own GPUs! These
  impressive speedups were achieved because FA V1 was the first to apply the concepts
  covered in this series of posts - tiling and streaming softmax - to the core Multi-Head
  Attention operation. Subsequent versions have employed much more sophisticated software
  features of GPUs, some of which only work on the most recent NVIDIA GPU generations,
  and have had varying degrees of involvement from Prof. Dao<sup id="fnref1"
    ><a href="#fn1">1</a></sup
  >, but the basic ideas introduced in V1 continue to be core and are what produced the
  original "great leap forward" in Multi-Head Attention performance on GPUs.
</p>

<h2>How FlashAttention fits into Multi-Head Attention</h2>

<p>
  FlashAttention is sometimes described as "a drop-in replacement for attention". That is
  not entirely accurate; it replaces <em>many</em> of the steps involved in Multi-Head
  Attention, but not all. Below is a reminder of all the steps involved in Multi-Head
  Attention:
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/01_multihead_attention_from_attention.png"
    alt="Standard Multi-Head Attention requires computing entire attention matrices within each head."
  />
</figure>

<p>
  FlashAttention fuses a particular key sequence of these operations into a single CUDA
  kernel; in particular, standard Multi-Head Attention requires "materializing" -
  defining and holding as one object in memory - entire "<code>sequence_length</code>
  &times; <code>sequence_length</code>" attention matrices within each head, whereas
  FlashAttention is able to compute the same final result while avoiding this
  memory-intensive materialization.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/02_2_flashattention_from_attention.png"
    alt="FlashAttention fuses several operations, avoiding ever materializing an attention matrix."
  />
</figure>

<p>
  Note that, for example, the final step of multiplying the concatenated outputs \(W_O\)
  is <em>not</em> part of the FlashAttention kernel.
</p>

<p>So how does it do it?</p>

<h2>FlashAttention v1: Preliminaries</h2>

<p>
  We'll start with three high level comments on FlashAttention that can keep you oriented
  when going through the specific steps of the algorithm:
</p>

<h3>
  1. The "tiling + accumulation" approach used in FlashAttention is like the one used in
  matrix multiplication, but with three input matrices rather than two.
</h3>

<p>
  In this <a href="/blog/cuda_matmul.html"><strong>blog post</strong></a
  >, we describe a technique for computing the output of an operation involving
  <em>two</em> input matrices by:
</p>

<ol>
  <li>Dividing the input matrices into "tiles".</li>
  <li>Successively feeding these into fast "shared" memory on the GPU</li>
  <li>
    For each set of tiles loaded in, do some computation to add this to an "accumulation"
    storing the results of all the prior tile computations in output
  </li>
</ol>

<p>
  The structure of what we do in FlashAttention is very similar (which is why reading
  through and making you understand that blog post is great scaffolding for understanding
  FlashAttention), except now we have <em>three</em> input matrices - \(Q\), \(K\), and
  \(V\) - and the operation we're trying to "emulate in a tiled fashion" is the attention
  operation:
</p>

\[ \operatorname{Attention}(Q, K, V) =
\operatorname{softmax}\!\left(\frac{QK^T}{\sqrt{d}}\right)V \]

<p>rather than simply a matrix multiplication.</p>

<h3>
  2. FlashAttention exploits that the same exact computation needs to happen within each
  batch and head element
</h3>

<p>
  That raises a question: where do these "two dimensional Tensors" we deal with in
  FlashAttention come from? \(Q\), \(K\), and \(V\), after all, are typically
  <em>four</em>-dimensional Tensors of shape
  <code>[batch_size, sequence_length, head_dim, num_heads]</code> in attention
  implementations; even if you discount the batch dimension--operations are always the
  same within each batch element in neural networks, so that they are actually
  "<code>batch_size</code>" identical operations happening in parallel--you are still
  left with three dimensional Tensors of shape:
  <code>[sequence_length, head_dim, num_heads]</code>. Here we take advantage of a
  further parallel structure in Multi-Head Attention: the computations
  <em>within each head</em> are identical. Naturally, then, in FlashAttention v1, we
  launch a GPU kernel with a grid of "<code>batch_size</code> * <code>num_heads</code>"
  thread blocks, each of which operates on a single
  <code>(batch_index, head_index)</code> tuple. Each thread block will then operate on
  the three "<code>sequence_length</code> &times; <code>head_dim</code>" Tensors:
</p>

<ul>
  <li>
    <code>Q[batch_index, head_index, :, :] </code>
  </li>
  <li>
    <code>K[batch_index, head_index, :, :] </code>
  </li>
  <li>
    <code>V[batch_index, head_index, :, :] </code>
  </li>
</ul>

<p>
  and produce the "<code>sequence_length</code> &times; <code>head_dim</code>" Tensor
  <code>O[batch_index, head_index, :, :]</code>. So, the algorithm we'll describe
  starting in the next section really is a "three 2D matrix" analogue of the "two 2D
  matrix" tiling algorithm described in the
  <a href="/blog/cuda_matmul.html">prior blog post</a>; "the magic of CUDA" (specifically
  being able to launch "<code>batch_size</code> * <code>num_heads</code>" blocks of
  threads at once) scales this up to be the 4D Tensor operation we need.
</p>

<p>
  Now that we understand that we can operate on two dimensional slices of \(Q\), \(K\),
  and, \(V\), we have to get to the core of the problem: how we are going to "tile" these
  to compute \(O\) without ever computing the full attention matrix.
</p>

<h3>
  3. The specific tiling of \(Q\), \(K\), and, \(V\) that works in FlashAttention comes
  from the dependencies within attention itself.
</h3>

<p>
  If you dive into a single row of the output matrix \(O\), you'll see that it
  <em>only</em> depends on the values from the corresponding row in \(Q\)! Incidentally,
  it turns out each element of this row will actually depend on <em>all</em> elements of
  \(K\) (due to the softmax operation), and all of the elements in the same column as
  that element within \(V\). Independently of this latter detail about \(K\) and \(V\),
  the point is that collectively the row needs to "see" all rows of \(K\) and \(V\)
  whereas it only needs to see the corresponding row of \(Q\). This leads to the tiling
  strategy described in the next section.
</p>

<h2>FlashAttention: Attention on Tiles</h2>

<blockquote>
  In the diagram and the text below, we'll use
  <ul>
    <li>\(N\) to refer to the <code>sequence_length</code></li>
    <li>\(D\) to refer to the <code>head_dim</code></li>
  </ul>

  <p>
    Moreover, we refer to a
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py"
      target="_blank"
      rel="noopener noreferrer"
    >
      Python re-write
    </a>
    of the
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/flash_attn_v1.cu"
      target="_blank"
      rel="noopener noreferrer"
    >
      CUDA kernel</a
    >
    that may be easier for readers to grok and/or hack on themselves than the CUDA C++
    code.
  </p>
</blockquote>

<p>
  As with the blog post where we covered matrix multiplication, the picture showing the
  geometry of what is going on is critical. The key picture is below:
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2026-01-08_flash_attention/04_tiled_flash_attention_2.png"
    alt="FlashAttention operates on tiles of Q, K, and V to produce accumulations of O_tile"
  />
</figure>

<ol class="fa-steps">
  <li>
    <p>
      <strong>Outer loop (over query tiles):</strong>
      We load in a tile of \(Tr\) rows of \(Q\) — call it \(Q_{tile}\).
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L47-L51"
        target="_blank"
        rel="noopener noreferrer"
        >Link to code</a
      >
    </p>
  </li>

  <li>
    <p>
      <strong
        ><a
          href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L60-L61"
          target="_blank"
          rel="noopener noreferrer"
          >Inner loop</a
        >
        (stream over key/value tiles while \(Q_{tile}\) is loaded into shared
        memory):</strong
      >
    </p>

    <ol class="fa-substeps">
      <li>
        <p>
          While this is loaded in, we
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L64-L69"
            target="_blank"
            rel="noopener noreferrer"
            >loop through</a
          >
          \(K\) and \(V\), \(Tc\) rows at a time (the <code>c</code> is for columns;
          you’ll see why shortly).
        </p>
      </li>

      <li>
        <p>
          For each \(K_{tile}\),
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L71-L93"
            target="_blank"
            rel="noopener noreferrer"
            >we take the dot product of \(Q_{tile}\) with \(K_{tile}^T\)</a
          >; this creates a "\(Tr\) rows by \(Tc\) columns" matrix of sums that, in the
          \(N \times N\) attention matrix, sits in the same <em>set of rows</em> as
          \(Q_{tile}\) and the same <em>set of columns</em> as \(K_{tile}\). We call this
          <code>sh_S</code>--<strong>s</strong>ums stored in <strong>sh</strong>ared
          memory--in the code. We
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L93"
            target="_blank"
            rel="noopener noreferrer"
            >record the maximum value</a
          >
          seen for each row in this block.
        </p>
      </li>

      <li>
        <p>
          We "rescale these sums", motivated by the ideas from the
          <a
            href="/blog/streaming_softmax.html"
            target="_blank"
            rel="noopener noreferrer"
            >streaming softmax blog post</a
          >. Remember that since softmax is itself a fraction, the attention operation
          (ignoring the scaling factor) can be written:
        </p>

        <div class="math-block">
          \[ \frac{\operatorname{scaled\_exponentials}(QK^T)\cdot V}
          {\operatorname{sum\_of\_scaled\_exponentials}(QK^T)} \]
        </div>

        <p>
          We know how to compute this in a "streaming" fashion: we update the running
          max, and use this updated max to <em>compute</em> the "scaled exponentials"
          themselves as well as adding to the running sum and rescaling it if necessary.
          These two operations happen
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L113"
            target="_blank"
            rel="noopener noreferrer"
            >here</a
          >
          and
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L117"
            target="_blank"
            rel="noopener noreferrer"
            >here</a
          >
          respectively. For an individual <code>row_in_q_tile</code> row, we store the
          running denominator in <code>row_sumexp[row_in_q_tile]</code> and the running
          max in <code>row_max[row_in_q_tile]</code> to be used later.
        </p>
      </li>

      <li>
        <p>
          We
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L124-L125"
            target="_blank"
            rel="noopener noreferrer"
            >multiply</a
          >
          these now-scaled rows by the corresponding \(V_{tile}\) to get a "<code
            >Tr</code
          >
          &times; <code>D</code>" tile contribution in the same location as \(O_{tile}\).
          In code we call this <code>O_tile_accum</code>.
        </p>
      </li>

      <li>
        <p>
          As in tiled matrix multiplication, we
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L126-L128"
            target="_blank"
            rel="noopener noreferrer"
            >increment</a
          >
          these "<code>Tr</code> &times; <code>D</code>" elements of \(O_{tile}\), then
          move on to the next \(K\) and \(V\) tiles.
        </p>
      </li>

      <li>
        <p>
          After looping through all tiles in \(K\) and \(V\), we’ve accumulated the
          <em>numerator</em> of softmax-dot for each row.
        </p>

        <p>
          So the final step is to
          <a
            href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L134-L139"
            target="_blank"
            rel="noopener noreferrer"
            >divide each row of the accumulated numerator by its stored denominator</a
          >, which we've stored as <code>row_sumexp[row_in_q_tile]</code>.
        </p>
      </li>
    </ol>
  </li>

  <li>
    <p>
      <strong>Back to the outer loop:</strong>
      <a
        href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/pseudocode_implementation.py#L43-L44"
        target="_blank"
        rel="noopener noreferrer"
        >Move on</a
      >
      to the next \(Q_{tile}\) and repeat until all rows of \(O\) are filled.
    </p>
  </li>
</ol>

<h2>Computational and memory complexity</h2>

<p>
  As should be clear from the diagrams and description of the algorithm above:
  <strong
    >FlashAttention actually consumes <em>constant</em> memory in the forward
    pass</strong
  >, whereas a naive Multi-Head Attention implementation would require \(O(N^2)\) memory!
  Though we won't go into the backward pass in detail in this blog post, it turns out the
  standard way of computing it, that offers a very good tradeoff between computation and
  memory, involves saving the per-row maxima and sums of scaled exponentials; this takes
  up \(O(N)\) memory. Still a huge improvement from \(O(N^2)\)!
</p>

<h2>Conclusion and Next Steps</h2>

<p>
  This post covered how tiling and streaming softmax enable us to compute large parts of
  the Multi-Head Attention operation without ever materializing the entire attention
  matrix, along with pseudocode that should make the algorithm more concrete. In a future
  blog post we'll walk through the CUDA side of the implementation in more detail; for
  now,
  <a
    href="https://github.com/SethHWeidman/ai_computing/blob/master/04_flash_attention/flash_attn_v1.cu"
    target="_blank"
    rel="noopener noreferrer"
    >this</a
  >
  should provide a clean implementation you can look through.
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup>
    <a href="#fnref1">1</a>
  </sup>
  <a
    href="https://pytorch.org/blog/flashattention-3/"
    target="_blank"
    rel="noopener noreferrer"
    >FlashAttention v3</a
  >
  (published on June 2024) was led by a team at
  <a href="https://research.colfax-intl.com" target="_blank" rel="noopener noreferrer"
    >Colfax Research</a
  >
  and NVIDIA, whereas FlashAttention v4 (which has only been semi-published, with Tri Dao
  giving a talk on an early version of it in August 2025) appears to be nearly 2,000
  lines of
  <a
    href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py"
    target="_blank"
    rel="noopener noreferrer"
    >Python CuTe DSL code</a
  >
  written
  <a
    href="https://github.com/Dao-AILab/flash-attention/blame/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py"
    target="_blank"
    rel="noopener noreferrer"
    >by Prof. Dao himself</a
  >.
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>
