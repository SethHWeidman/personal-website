<h1 class="post-title">Attention, Single and Multi-Head</h1>
<p class="post-date">December 30, 2025</p>

<figure class="post-banner">
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/banner-image.png"
    alt="Hand-drawn overview diagram of self-attention and multi-head attention."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  Much can and has been written on attention, and for good reason: it can be
  intimidating! The original paper that described the "Multi-Head Attention" mechanism
  now ubiquitous in LLMs,
  <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a
  >, is ten technical pages, and originally shipped with an implementation in the
  powerful but notoriously difficult TensorFlow framework (now buried in
  <a
    href="https://github.com/tensorflow/tensor2tensor"
    target="_blank"
    rel="noopener noreferrer"
    >this archived GitHub repo</a
  >) - even when folks published simplified PyTorch implementations, the 4D Tensor
  reshapings were hard to map to the operations and diagrams from the paper. Several
  technical writers published deep dives breaking these simplified steps down even
  further; despite this, in my experience, few people can give a "whiteboard-level"
  overview of the different components of Attention and how they fit together. Thus, this
  post adds yet another attention explainer to this universe; the goal is that readers
  <em>will</em> be able to understand not only the high level structure of Attention, but
  also where FlashAttention fits in.
</p>

<h2>Quick background on attention</h2>

<p>
  Attention is used to process sequences, specifically sequences where each element is
  represented as a vector, as is the case with language models where the sequence of
  vectors represents a sequence of tokens. Like other sequence-modeling building blocks
  (historically: RNNs, and their evolved variants, LSTMs and GRUs<sup id="fnref1"
    ><a href="#fn1">1</a></sup
  >), Attention blocks take in a sequence of token vectors and output a sequence of new,
  “refined” token vectors. So on one level, Attention is yet another composable neural
  network block, as with its precursors. Indeed, the
  <em>Attention Is All You Need</em> paper showed that Multi-Head Attention worked well
  specifically inside a broader “sequence-to-sequence” architecture they called a
  <strong>Transformer</strong>. If you want to see what this looks like in
  straightforward PyTorch code,
  <a
    href="https://github.com/SethHWeidman/LLMs-from-scratch/blob/main/ch04/01_main-chapter-code/gpt.py#L167-L182"
    target="_blank"
    rel="noopener noreferrer"
    >here</a
  >
  is a clean Transformer implementation; the <code>x = self.att(x)</code> line applies
  multi-head attention.
</p>

<p>So what actually happens inside one of these "Attention blocks"?</p>

<h2>Inside an attention block</h2>

<p>
  We focus here on Attention blocks that are part of LLMs, with the goal of helping the
  LLMs generate "good next tokens" given a sequence of prior tokens. We want these blocks
  to be able to generate an updated representation for each input token, after the block,
  that can optionally “take into consideration” or, to use the term commonly used in
  Attention, "<em>attend to</em>" each of the prior tokens<sup id="fnref2"
    ><a href="#fn2">2</a></sup
  >. Attention achieves this with an approach loosely motivated by database retrieval: it
  starts by creating three different representations of each token -
  <strong>queries (Q)</strong>, <strong>keys (K)</strong>, and
  <strong>values (V)</strong> - using three distinct sets of learnable parameters
  (allowing the representations themselves to be refined as the model is trained).
  Queries and keys together let the model learn, for each token:
  <strong
    >“of all the tokens that came before this token, including this one, which tokens
    should I ‘pay attention to’?”</strong
  >
  These quantities are known as the <strong>attention weights</strong> for that token;
  they sum to 1. The final output vector of each token, after Attention, is
  <strong
    >a weighted average of these learned Values, weighted by the attention weights (which
    were generated by the queries and keys)</strong
  >.
</p>

<p>
  Note: Because the Queries, Keys, and Values are all derived from the same input
  sequence in the context of LLMs, this variant of Attention is called
  <strong>Self-Attention</strong>; this is how we refer to it in the following diagrams.
</p>

<p>
  Below is a diagram that summarizes this; each thick black line from an element
  indicates that that element is an input to a matrix multiplication:
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/01-self-attention.png"
    alt="Hand-drawn diagram of self-attention: input vectors produce queries/keys/values, which form attention weights that combine values into output vectors."
  />
</figure>

<h2>Multi-head attention</h2>

<p>
  The Attention is All You Need paper didn't just suggest using one big Self-Attention
  operation; it proposed a variant known as
  <strong>Multi-Head Attention</strong>. Mechanically, it works like this:
</p>

<ol>
  <li>
    Each input token vector is split into chunks<sup id="fnref3"
      ><a href="#fn3">3</a></sup
    >. We interpret each chunk as an "attention head".
  </li>
  <li>
    We do the attention steps described above on each head <em>independently</em>. Each
    “head” - which is just a slice of the input vectors - gets its own queries, keys, and
    values, and uses these to compute its own attention weights and ultimately its own
    output vectors; that is, it does Self-Attention, but on just a slice of the input
    vectors.
  </li>
  <li>We concatenate these "attention head outputs" back together.</li>
  <li>
    We do one last step to let the model's final representation of each token be a
    “learned mix” of the concatenated attention head outputs.
  </li>
</ol>

<p>
  The intuition behind trying Multi-Head Attention is that each head could learn
  independent aspects of the language we are trying to model; one head could focus on
  grammar in the current sentence, while another could focus on logical consistency with
  what tokens from long before. It's a bit speculative whether that actually happens
  within Multi-Head Attention, but empirically, the authors found that Multi-Head
  Attention did perform better than just doing a one big Single-Head Attention
  <sup id="fnref4"><a href="#fn4">4</a></sup> operation. This has been corroborated many
  times since, and Multi-Head Attention (and variants thereof<sup id="fnref4"
    ><a href="#fn4">5</a></sup
  >) is still used within leading LLMs today.
</p>

<p>
  Below is a diagram of Multi-Head Attention in terms of Self-Attention; as before,
  arrows with solid indicate that the elements are inputs to matrix multiplications, and
  newly, arrows with dotted lines indicate splitting or concatenating.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/02-multi-head-attention.png"
    alt="Hand-drawn diagram of multi-head attention: token vectors split into heads, each head runs self-attention, outputs are concatenated and mixed into final output vectors."
  />
</figure>

<h2 id="implementation">Implementations</h2>

<p>
  Here are a few implementations with code comments indicating which lines of code
  correspond to which steps in the diagrams above:
</p>

<ul>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L34-L55"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is an implementation of <strong>Single-Head Attention</strong>
  </li>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L93-L103"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is an implementation of <strong>Multi-Head Attention</strong>, naively implemented as
    several single-head modules whose outputs are concatenated and mixed.
  </li>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L133-L143"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is a more standard PyTorch implementation of Multi-Head Attention: it computes each
    the queries, keys, and values for all heads in just one operation each, reshapes the
    keys and queries to compute all the attention weights for all heads at once, and
    finally does another reshaping followed by the mixing of the output vectors across
    heads.
  </li>
</ul>

<h2>Attention's Bottleneck and the Motivation for FlashAttention</h2>

<p>
  Much has been written about the computational cost of attention. Here we'll just
  mention the key advantage, from a computational perspective, of attention over its
  precursors, as well as its largest downside - a downside which FlashAttention is
  designed to mitigate.
</p>

<ul>
  <li>
    The bottleneck of the actual computations in attention precursors such as LSTMs was
    that they had to be processed sequentially: you couldn't compute the representation
    for token \(t\) until you had computed all prior tokens, 1 to \(t-1\). By contrast,
    attention computes representations for all tokens at once, allowing parallelism in
    implementations.
  </li>
  <li>
    The bottleneck of the computation of Attention is...attention: that is, computing,
    and especially storing in memory, the "<code>sequence_length</code> by
    <code>sequence_length</code>" attention matrix itself (the red one in the banner
    image above). For a 10K token sequence, that's a matrix with 100 million elements,
    which is 200-400 MB in memory (depending on precision), for a single attention layer;
    bleeding edge LLMs now advertise 1M token context lengths.
  </li>
</ul>

<p>
  This memory bottleneck is exactly what <strong>FlashAttention</strong> targets. Working
  within the memory hierarchy of GPUs, using a very similar tiling strategy to the one
  described <a href="https://www.sethweidman.com/blog/cuda_matmul.html">here</a>, as well
  as the elegant "<a href="https://www.sethweidman.com/blog/streaming_softmax.html"
    >streaming softmax</a
  >" trick, it brings the attention computation down from \(O(L^2)\) memory to \(O(L)\)
  memory. Without tricks like FlashAttention, we might not be able to have LLMs with
  100K+ token context windows! These tricks are truly enablers of the AI age in which
  we're living; they even power the LLMs that helped the author learn this subject and
  write this blog post that you are now reading, the subject of which is...them! They
  thus recursively help make the world smarter.
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup>
    <a href="#fnref1">1</a>
  </sup>
  For an overview of RNNs, LSTMs, and GRUs, see Chapter 6 of my book,
  <a
    href="https://www.amazon.com/Deep-Learning-Scratch-Building-Principles/dp/1492041416"
    target="_blank"
    rel="noopener noreferrer"
    >Deep Learning from Scratch</a
  >.
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>

<p id="fn2" class="footnote">
  <sup>
    <a href="#fnref2">2</a>
  </sup>
  This "prior tokens only" claim is true for decoder-style Transformers used for
  next-token prediction (LLMs). In encoder-style Transformers such as BERT, where the
  goal is to create a representation of the entire sequence, tokens can attend to future
  tokens as well.
  <a href="#fnref2" class="footnote-backref">↩</a>
</p>

<p id="fn3" class="footnote">
  <sup>
    <a href="#fnref3">3</a>
  </sup>
  Concretely, if the model dimension is <code>d_model</code> and there are
  <code>n_heads</code> heads, each token vector is reshaped into
  <code>n_heads</code> chunks of size <code>d_head = d_model / n_heads</code>.
  <a href="#fnref3" class="footnote-backref">↩</a>
</p>

<p id="fn4" class="footnote">
  <sup>
    <a href="#fnref4">4</a>
  </sup>
  In comparisons to multi-head attention, self-attention is often referred to as
  "single-head attention"; this is why in code you'll see that the class name for this
  operation is <code>SingleHeadAttention</code>
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<p id="fn5" class="footnote">
  <sup>
    <a href="#fnref5">5</a>
  </sup>
  Sebastian Raschka does a great job covering several of these in the "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/04_gqa"
    target="_blank"
    rel="noopener noreferrer"
    >04</a
  >", "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/05_mla"
    target="_blank"
    rel="noopener noreferrer"
    >05</a
  >", "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/06_swa"
    target="_blank"
    rel="noopener noreferrer"
    >06</a
  >", and "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/07_moe"
    target="_blank"
    rel="noopener noreferrer"
    >07</a
  >" sections in the bonus content for Chapter 4 of his book.
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<br />
<button onclick="window.location.href='/'">Back to Home</button>
