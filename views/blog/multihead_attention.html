<h1 class="post-title">Attention, Single and Multi-Head</h1>
<p class="post-date">December 30, 2025</p>

<figure class="post-banner">
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/banner-image.png"
    alt="Hand-drawn overview diagram of self-attention and multi-head attention."
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  Much can and has been written on attention, yet deep understanding of its inner
  workings is still rare. The original paper that described the "Multi-Head Attention"
  mechanism now ubiquitous in LLMs,
  <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a
  >, is ten technical pages, and originally shipped with an implementation in the
  powerful but notoriously difficult TensorFlow framework (now buried in
  <a
    href="https://github.com/tensorflow/tensor2tensor"
    target="_blank"
    rel="noopener noreferrer"
    >this archived GitHub repo</a
  >) - even when folks published simplified PyTorch implementations, the 4D Tensor
  reshapings were hard to map to the operations and diagrams from the paper. Several
  technical writers published deep dives breaking these simplified steps down even
  further; nevertheless, in my experience, few can give even a "whiteboard-level"
  overview of Attention. Thus, this post adds yet another attention explainer to this
  universe, making sure we understand the high level structure and motivating where
  FlashAttention fits in.
</p>

<h2>Quick background on attention</h2>

<p>
  Attention is used to process sequences, specifically sequences where each element is
  represented as a vector, as is the case with language models where the sequence of
  vectors represents a sequence of tokens. Like other sequence-modeling building blocks
  (historically: RNNs, and their evolved variants, LSTMs and GRUs<sup id="fnref1"
    ><a href="#fn1">1</a></sup
  >), Attention blocks take in a sequence of token vectors and output a sequence of new,
  “refined” token vectors. So on one level, Attention is yet another composable neural
  network block, with the same precursors. Indeed, the
  <em>Attention Is All You Need</em> paper showed that multi-head attention worked well
  specifically inside a broader “sequence-to-sequence” architecture they called a
  <strong>Transformer</strong>. If you want to see what this looks like in
  straightforward PyTorch code, you can see a clean Transformer implementation
  <a
    href="https://github.com/SethHWeidman/LLMs-from-scratch/blob/main/ch04/01_main-chapter-code/gpt.py#L167-L182"
    target="_blank"
    rel="noopener noreferrer"
    >here</a
  >; the <code>x = self.att(x)</code> line applies multi-head attention.
</p>

<p>So what actually happens inside one of these "Attention blocks"?</p>

<h2>Inside an attention block</h2>

<p>
  Given that they will be used inside LLMs, we want each Attention block to be able to
  generate an updated representation for each input token, after the block, that can
  optionally “take into consideration” or, to use the term commonly used in Attention,
  "<em>attend to</em>" each of the prior tokens<sup id="fnref2"><a href="#fn2">2</a></sup
  >. Attention achieves this with an approach loosely motivated by database retrieval: it
  starts by creating three different representations of each token, using three distinct
  sets of learnable parameters (allowing these representations to be refined as the model
  is trained):
</p>

<ul>
  <li>
    <strong>Queries (Q)</strong> and <strong>Keys (K)</strong>: these together let the
    model learn, for each token: “of all the tokens that came before this token,
    including this one, which tokens should I ‘pay attention to’?” These quantities are
    known as the <strong>attention weights</strong> for that token; they sum to 1.
  </li>
  <li>
    The third learned representation is known as the <strong>Values (V)</strong>; the
    final output vector, of each token, after Attention, is a weighted average of these
    Values, weighted by the attention weights.
  </li>
</ul>

<p>
  Note: Because the Queries, Keys, and Values are all derived from the same input
  sequence in the context of LLMs, this variant of Attention is called
  <strong>Self-Attention</strong>.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/01-self-attention.png"
    alt="Hand-drawn diagram of self-attention: input vectors produce queries/keys/values, which form attention weights that combine values into output vectors."
  />
  <figcaption>
    <em>
      Self-attention in one picture: build Q/K/V from the inputs, compute attention
      weights from QK<sup>T</sup>, and compute the output vectors by weighting V with
      these attention weights.
    </em>
  </figcaption>
</figure>

<h2>Multi-head attention</h2>

<p>
  The Attention is All You Need paper didn't just suggest using one big single
  Self-Attention operation; it proposed a variant known as
  <strong>Multi-Head Attention</strong>. Mechanically, it works like this:
</p>

<ol>
  <li>
    Each input token vector is split into chunks<sup id="fnref3"
      ><a href="#fn3">3</a></sup
    >. We interpret each chunk as an "attention head".
  </li>
  <li>
    We do the attention steps described above on each head <em>independently</em>. Each
    “head” - which is just a slice of the input vectors - gets its own queries, keys, and
    values, and uses these to compute its own attention weights and ultimately its own
    output vectors; that is, it does Self-Attention, but on just a slice of the input
    vectors.
  </li>
  <li>We concatenate these attention head outputs back together.</li>
  <li>
    We do one last step to let the model's final representation of each token be a
    “learned mix” of the concatenated attention head outputs.
  </li>
</ol>

<p>
  The intuition behind trying Multi-Head Attention is that each head could learn
  independent aspects of the language we are trying to model; one head could focus on
  grammar in the current sentence, while another could focus on logical consistency with
  what tokens from long before. It's a bit speculative whether that actually happens
  within Multi-Head Attention, but empirically, the authors found that Multi-Head
  Attention did perform better than just doing a one big Single-Head Attention
  <sup id="fnref4"><a href="#fn4">4</a></sup> operation. This has been corroborated many
  times since, and Multi-Head Attention (and variants thereof<sup id="fnref4"
    ><a href="#fn4">4</a></sup
  >) is still used within leading LLMs today.
</p>

<p>Here's a diagram of what that looks like:</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-12-29_attention/02-multi-head-attention.png"
    alt="Hand-drawn diagram of multi-head attention: token vectors split into heads, each head runs self-attention, outputs are concatenated and mixed into final output vectors."
  />
  <figcaption>
    <em>
      Multi-head attention: attention is computed independently in each head, then
      concatenated and mixed to produce the final output vectors.
    </em>
  </figcaption>
</figure>

<h2 id="implementation">Implementations</h2>

<ul>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L34-L55"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is an implementation of <strong>Single-Head Attention</strong>
  </li>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L93-L103"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is an implementation of <strong>Multi-Head Attention</strong>, naively implemented as
    several single-head modules whose outputs are concatenated and mixed.
  </li>
  <li>
    <a
      href="https://github.com/SethHWeidman/ai_computing/blob/master/05_multihead_attention/compare_implementations.py#L133-L143"
      target="_blank"
      rel="noopener noreferrer"
      >Here</a
    >
    is a more standard PyTorch implementation of Multi-Head Attention: it computes each
    the queries, keys, and values for all heads in just one operation each, reshapes the
    keys and queries to compute all the attention weights for all heads at once, and
    finally does another reshaping followed by the mixing of the output vectors across
    heads.
  </li>
</ul>

<h2>Attention's Bottleneck and the Motivation for FlashAttention</h2>

<p>
  Much has been written about the computational cost of attention. Here we'll just
  mention the key advantage, from a computational perspective, of attention over its
  precursors, as well as its largest downside - a downside which FlashAttention is
  designed to mitigate.
</p>

<ul>
  <li>
    The bottleneck of the actual computations in attention precursors such as LSTMs was
    that they had to be processed sequentially: you couldn't compute the representation
    for token \(t\) until you had computed all prior tokens, 1 to \(t-1\). By contrast,
    attention computes representations for all tokens at once, in parallel.
  </li>
  <li>
    The bottleneck of the computation of Attention is...attention. Namely, computing, and
    especially storing in memory, the "sequence_length x sequence_length" attention
    matrix itself (the red one in the banner image above). For a 10K token sequence,
    that's a matrix with 100 million elements, which is 200-400 MB in memory (depending
    on precision), for a single attention layer; bleeding edge LLMs now advertise 1M
    token context lengths.
  </li>
</ul>

<p>
  This memory bottleneck is exactly what <strong>FlashAttention</strong> targets. Working
  within the memory hierarchy of GPUs, using a very similar tiling strategy to the one
  described <a href="https://www.sethweidman.com/blog/cuda_matmul.html">here</a>, as well
  as the elegant "<a href="https://www.sethweidman.com/blog/streaming_softmax.html"
    >streaming softmax</a
  >" trick, it brings the attention computation down from \(O(L^2)\) memory to \(O(L)\)
  memory. Without tricks like FlashAttention, we might not be able to have LLMs with
  100K+ token context windows! These tricks are truly enablers of the AI age in which
  we're living; they even power the LLMs that helped the author learn this subject and
  write this blog post that you are now reading, about them. They thus recursively help
  make the world smarter.
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup>
    <a href="#fnref1">1</a>
  </sup>
  For an overview of RNNs, LSTMs, and GRUs, see Chapter 6 of my book,
  <a
    href="https://www.amazon.com/Deep-Learning-Scratch-Building-Principles/dp/1492041416"
    target="_blank"
    rel="noopener noreferrer"
    >Deep Learning from Scratch</a
  >.
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>

<p id="fn2" class="footnote">
  <sup>
    <a href="#fnref2">2</a>
  </sup>
  This "prior tokens only" claim is true for decoder-style Transformers used for
  next-token prediction (LLMs). In encoder-style Transformers such as BERT, where the
  goal is to create a representation of the entire sequence, tokens can attend to future
  tokens as well.
  <a href="#fnref2" class="footnote-backref">↩</a>
</p>

<p id="fn3" class="footnote">
  <sup>
    <a href="#fnref3">3</a>
  </sup>
  Concretely, if the model dimension is <code>d_model</code> and there are
  <code>n_heads</code> heads, each token vector is reshaped into
  <code>n_heads</code> chunks of size <code>d_head = d_model / n_heads</code>.
  <a href="#fnref3" class="footnote-backref">↩</a>
</p>

<p id="fn4" class="footnote">
  <sup>
    <a href="#fnref4">4</a>
  </sup>
  In comparisons to multi-head attention, self-attention is often referred to as
  "single-head attention"; this is why in code you'll see that the class name for this
  operation is <code>SingleHeadAttention</code>
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<p id="fn5" class="footnote">
  <sup>
    <a href="#fnref5">5</a>
  </sup>
  Sebastian Raschka does a great job covering several of these in the "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/04_gqa"
    target="_blank"
    rel="noopener noreferrer"
    >04</a
  >", "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/05_mla"
    target="_blank"
    rel="noopener noreferrer"
    >05</a
  >", "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/06_swa"
    target="_blank"
    rel="noopener noreferrer"
    >06</a
  >", and "<a
    href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/07_moe"
    target="_blank"
    rel="noopener noreferrer"
    >07</a
  >" sections in the bonus content for Chapter 4 of his book.
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<br />
<button onclick="window.location.href='/'">Back to Home</button>
