<h1 class="post-title">Tiling Matrix Multiplication on the GPU</h1>
<p class="post-subtitle">Using CUDA and Shared Memory</p>
<p class="post-date">December 3, 2025</p>
<script src="/mathjax-config.js"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>

<!-- Open Graph tags (for LinkedIn, etc.) -->
<meta property="og:title" content="Tiling Matrix Multiplication on the GPU" />
<meta
  property="og:description"
  content="An illustrated walkthrough of how CUDA shared-memory tiling speeds up matrix multiplication on the GPU."
/>
<meta property="og:url" content="https://www.sethweidman.com/blog/cuda_matmul.htm" />
<meta
  property="og:image"
  content="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/banner-image.png"
/>
<meta property="og:type" content="article" />

<!-- Twitter Card tags (for X) -->
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Tiling Matrix Multiplication on the GPU" />
<meta
  name="twitter:description"
  content="An illustrated walkthrough of how CUDA shared-memory tiling speeds up matrix multiplication on the GPU."
/>
<meta
  name="twitter:image"
  content="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/banner-image.png"
/>

<figure class="post-banner">
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/banner-image.png"
    alt="Illustration of CUDA shared-memory tiling for matrix multiplication"
    class="wide-image"
    loading="eager"
  />
</figure>

<p>
  CUDA is an important part of the AI stack, allowing programmers to use NVIDIA GPUs to
  their full capacity. This blog post, targeted at developers who mostly code at the
  Python / PyTorch level, will go into "the first couple layers" of CUDA, explaining what
  it is and how it allows programmers to write programs that take advantage of the unique
  properties of NVIDIA GPUs. In particular, we'll illustrate one very common structure
  for CUDA programs, a structure that
  <a
    href="https://github.com/Dao-AILab/flash-attention"
    target="_blank"
    rel="noopener noreferrer"
    >FlashAttention</a
  >
  uses in a similar but more sophisticated way. We'll also explain why this structure
  aligns with NVIDIA GPUs' hardware and how it can be used in a beautiful way to speed up
  matrix multiplication.
</p>

<h2>Background</h2>

<p>
  Writing programs in a “GPU-aware” way - that is, aware that you have a specialized
  processor called a GPU at your disposal - is different than other programming
  paradigms, such as writing a Python script designed to run on a single core. At the
  highest level, you must be aware that by default, your program will run on a CPU
  (referred to in CUDA-land as the
  <em>host</em>), whereas you can write special functions called <em>kernels</em> and
  explicitly launch those functions on a GPU (referred to in CUDA-land as a
  <em>device</em>). Before diving into the code, we'll cover the key pieces of the mental
  model you should have for what your "GPU" is, what it can do, and how it is different
  than your CPU.
</p>

<h3>Hardware Background: GPUs</h3>

<p>
  You can think of the GPU as having many processors, orders of magnitude more in
  quantity than what your CPU has, each of which is optimized for “running many simple
  computations in parallel”. By contrast, your CPU has fewer processors / cores, each of
  which has more functionality and overall power but simply isn't as optimized for
  "extreme parallelism" as a GPU. For example, the M4 chip on the MacBook Pro on which
  I'm writing this blog post has 10 cores, whereas the L4 GPU I used to run the
  experiments described below has about 7,000 cores. These 7,000 cores come from 58
  distinct hardware chips NVIDIA calls "Streaming Multiprocessors (SMs)" all of which are
  present on a single GPU; each SM has 128 cores that are designed to perform simple
  computations quickly (so to be exact, the L4 GPU has \(58 \times 128 = 7,424\) cores).
</p>

<p>
  Thus, one should keep in mind when writing GPU-aware programs that any object can be
  stored and read from one of three<sup id="fnref1"><a href="#fn1">1</a></sup> types of
  memory:
</p>

<ol>
  <li>“Host memory”: the memory of the CPU</li>
  <li>“Global memory”: the memory “globally” on the GPU</li>
  <li>
    “Shared memory”: memory that has been loaded onto an individual SM - you’ll
    understand shortly why it is called “shared”
  </li>
</ol>

<p>
  Given that the computations will actually take place on the individual SMs, writing
  programs that require the same amount of <em>computation</em> in a way that they
  require either:
</p>

<ul>
  <li>Fewer reads from host memory to global memory</li>
  <li>Fewer reads from global memory to shared memory</li>
</ul>

<p>
  can result in significant speedups. The "meat" of this blog post is a well-known
  algorithm for doing
  <a href="#matrix-multiplication">matrix multiplication</a>
  while reducing the number of reads from <em>global memory</em> to
  <em>shared memory</em>
  by an order of magnitude over the naive approach. But before we get there, we have to
  cover some fundamentals about GPU software.
</p>

<h3>Software Background</h3>

<p>
  CUDA - technically what we'll describe here is “CUDA C++”, an extension of C++ that has
  additional keywords like <code>__global__</code> and <code>__shared__</code> - programs
  run via "CUDA kernels": these are special C++ functions designed to run on GPUs.
  Understanding the level at which these CUDA kernels actually run is subtle, as they
  actually operate at multiple levels at once.
</p>
<p>
  The actual “atomic units” that do work within SMs, at a software level, are
  <em>threads</em>. To “launch a CUDA kernel”, we don't merely say "do this work within
  each thread" (we could do this, but we would lose out on a lot of the power of these
  kernels); instead, we launch it with a <em>group of blocks of threads</em> known as a
  <em>grid</em>. To make this happen, we pass in two special keywords into the kernel
  before we pass in the the regular function arguments: the number of blocks of threads
  to launch, and the number of threads within each block. See the code block
  <a
    href="https://github.com/SethHWeidman/intro_to_cuda/blob/main/demo3_matmul/matmul.cu#L159"
    target="_blank"
    rel="noopener noreferrer"
    >here</a
  >
  for an example of this kernel launch<sup id="fnref2"><a href="#fn2">2</a></sup
  >. Conceptually, this line
</p>

<blockquote>
  <p>
    <code
      >mm_kernel&lt;&lt;&lt;blocks_per_grid, threads_per_block&gt;&gt;&gt;(d_U, d_V, d_V,
      d_W_GPU, N);</code
    >
  </p>
</blockquote>

<p>says:</p>

<blockquote>
  <p>
    "launch a function that will run on the SMs on a GPU, in a grid of
    <code>blocks_per_grid</code> blocks, with <code>threads_per_block</code> threads per
    block, and arguments <code>d_U</code>, <code>d_V</code>, <code>d_W_GPU</code>, and
    <code>N</code>"
  </p>
</blockquote>

<p>
  The consequence of all this is that there are three “levels” of computation happening
  when we launch a CUDA kernel on a GPU, each of which the programmer must keep in mind:
</p>

<ul>
  <li>Individual threads are doing computations</li>
  <li>These threads are grouped into "blocks of threads"</li>
  <li>These "blocks of threads" are grouped into a grid</li>
</ul>

<p>The CUDA kernel is operating across all three of these levels:</p>

<ul>
  <li>The kernel itself is replicated across and runs in each thread.</li>
  <li>
    Special variables are accessible inside the kernel indicating:
    <ul>
      <li>The total number of blocks: <code>blockDim</code></li>
      <li>
        The thread's block’s position within the grid of blocks: <code>blockIdx</code>
      </li>
      <li>The thread’s position within its block: <code>threadIdx</code></li>
    </ul>
  </li>
  <li>Kernels must of course what each thread should do.</li>
  <li>
    <strong>Critical point</strong>: kernels can <em>also</em> define and access objects
    that are accessible by <em>all threads within a block of threads</em>. Going back to
    the three layers of memory above, such objects live in
    <strong>“shared” memory</strong>, on the SM. We now know why this is called “shared
    memory”; the memory is shared by a block of threads.<sup id="fnref3"
      ><a href="#fn3">3</a></sup
    >
  </li>
</ul>

<h3>The Common Pattern</h3>

<p>
  This leads to a common pattern seen in GPU programming: instead of repeatedly loading
  data from global memory on the GPU into shared memory right before it is needed for
  computation, we:
</p>

<ol>
  <li>
    First load an entire chunk of data into shared memory that is then accessible within
    a block of threads.
  </li>
  <li>
    Have each thread fill some component of those shared objects; threads do this work in
    parallel
  </li>
  <li>
    Sync the threads - wait until each thread has finished filling its component of the
    shared objects. CUDA C++ has a special <code>__syncthreads()</code> function for
    this.
  </li>
  <li>Have each thread do some operation involving the now-filled shared objects.</li>
</ol>

<p>
  This pattern can allow for an order of magnitude fewer reads from “global memory” (on
  the GPU) which are relatively slow compared to reading from “shared memory” (on the
  SM). Now we have all the scaffolding to explain how this pattern can be applied to
  matrix multiplication!
</p>

<h2 id="matrix-multiplication">Matrix multiplication</h2>

<p>We’ll proceed assuming you understand the algorithm for matrix multiplication.</p>

<p>
  Let’s assume we can read all of the two matrices we want to multiply into memory on a
  single GPU; recall that this means the matrix is in “global memory” on the GPU; given
  that entire small language models (single digit billions of parameters) are read into
  single GPUs, this isn't such a crazy assumption.<sup id="fnref4"
    ><a href="#fn4">4</a></sup
  >
  For simplicity, let’s assume both matrices are \(N \times N\).
</p>

<p>
  Suppose we want to compute “\(B \times B\) block” of elements, where \(B\) is an
  integer that divides \(N\). How many reads from global memory, into shared memory,
  would we have to make to do this naively?
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/01-block-and-slices.png"
    alt="Diagram for the B x B block and the corresponding row and column slices."
  />
</figure>

<p>
  The answer is straightforward: to compute an individual element, you’d need to read in
  \(2N\) elements, and take the dot product of their vectors. You’d need to do this \(B
  \times B\) times, once for each of the \(B^2\) elements in the \(B \times B\) block. So
  there are a total of <strong>\(2NB^2\)</strong> reads from global memory into shared
  memory.
</p>

<p>
  Now, we’ll walk through a way of using shared memory to do the same matrix
  multiplication with \(B \times\) fewer reads from global memory into shared memory! At
  the end, we’ll show using a simple experiment that this does in fact make the
  multiplication faster.
</p>

<h3>Step 1:</h3>

<p>
  Consider the \(B \times B\) block at the top left of the matrix - call it “A”. Divide
  the “\(B \times N\)” row containing A into \(K\) blocks (by construction, \(K =
  \frac{N}{B}\)).
</p>

<p>
  Divide the column containing A, \(N\) rows by \(B\) columns into \(K\) chunks
  similarly.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/02-row-col-tiling.png"
    alt="Diagram for Step 1 showing the B x N row slice and N x B column slice divided into
  K blocks."
  />
</figure>

<h3>Step 2:</h3>

<p>
  Launch many threads, and use these threads to load the first “tile” - “tile” is the
  technical name for these “chunks” - of each of the “row slice” and “column slice” into
  shared memory. Do this in parallel using all the threads; when finished, we will have a
  \(B \times B\) "row tile" and a \(B \times B\) "column tile" loaded into
  <em>shared</em> memory. Finally, "sync threads", ensuring that the entire "row tile"
  and "column tile" are loaded into shared memory before proceeding.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/03-tiles-in-shared-memory.png"
    alt="Diagram for Step 2 showing the first tiles loaded into shared memory."
  />
</figure>

<h3>Step 3:</h3>

<p>
  Here’s the clever and beautiful part: use these two tiles to increment, though not
  fully compute, each of the \(B \times B\) sums in A.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/04-partial-sums-in-block.png"
    alt="Diagram for Step 3 showing partial sums within the B x B block."
  />
</figure>

<p>
  For example, to increment the element in the second row, first column of A, we perform
  the dot product of the second row of the “row tile” with the first column of the
  “column tile”, and add it to the ongoing sum of that element.
</p>

<p>
  After this step, "sync threads" a second time, ensuring that all partial sums have been
  incremented.
</p>

<h3>Step 4, and beyond</h3>

<p>
  Move on to the next “row tile” in the row containing A and the next “column tile” in
  the column containing A (if we were incrementing a tile counter \(k\) started at 1,
  we’d be incrementing it to 2). We’d then perform the same operations as in step 3,
  incrementing each of the \(B^2\) elements of A.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/05-next-tile-iteration.png"
    alt="Diagram for Step 4 showing moving to the next pair of tiles."
  />
</figure>

<h3>Step 5</h3>

<p>
  Repeat this process for \(k = 3, 4,\ldots,K\)! By the time we’ve iterated through all
  \(K\) tiles, we’ve computed the full value of all the elements of A!<sup id="fnref5"
    ><a href="#fn5">5</a></sup
  >
</p>

<h2>Analysis</h2>

<p>
  We read in a total of \(2B^2\) elements on each of \(K = \frac{N}{B}\) iterations, for
  a total “global -&gt; shared memory cost” of \(\frac{2NB^2}{B} = 2BN\). This is \(B\)
  times less than doing the reads naively!
</p>

<h2>Implementation and Benchmarking</h2>

<p>
  I have an implementation
  <a
    href="https://github.com/SethHWeidman/intro_to_cuda/blob/main/demo3_matmul/matmul_kernels.cuh#L27"
    target="_blank"
    rel="noopener noreferrer"
    ><strong>here</strong></a
  >. I would not have been able to write it without referencing:
</p>

<ul>
  <li>
    <a
      href="https://github.com/stanford-cs149/intro_to_cuda/blob/main/demo3_matmul/matmul.cu"
      target="_blank"
      rel="noopener noreferrer"
      >This implementation</a
    >
    from the GitHub page of Stanford’s CS149 (Parallel Computing) course (my
    implementation is in my fork of this repo).
  </li>
  <li>
    <a
      href="https://leimao.github.io/blog/CUDA-Matrix-Multiplication/"
      target="_blank"
      rel="noopener noreferrer"
      >This blog post</a
    >
    from Lei Mao (now an engineer at Meta; he was at NVIDIA when he wrote this)
  </li>
</ul>

<p>
  There’s a lot going on in the implementation, but notice that
  <a
    href="https://github.com/SethHWeidman/intro_to_cuda/blob/main/demo3_matmul/matmul_kernels.cuh#L29-L30"
    target="_blank"
    rel="noopener noreferrer"
    >these lines</a
  >
  are the ones where two “tiles”, each 2D arrays of size
  <code>BLOCK_DIM</code> &times; <code>BLOCK_DIM</code> are initialized, using the
  <code>__shared__</code> CUDA C++ keyword. In addition, in line with the explanation
  above, you'll see two uses of <code>__syncthreads()</code>. Other elements of the
  implementation, such as the use of <code>threadIdx.x</code> and
  <code>threadIdx.y</code> to make the implementation more elegant, could be worth
  another blog post!
</p>

<h3>Benchmarking against the naive approach</h3>

<p>
  The
  <a
    href="https://github.com/SethHWeidman/intro_to_cuda/blob/main/demo3_matmul/README.md"
    target="_blank"
    rel="noopener noreferrer"
    >README</a
  >
  shows about a 40% speedup in using the shared memory approach vs. the naive approach
  (even if the naive approach still uses parallelism) - 553 ms vs. 900 ms!
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/plots/01-kernel-timings.png"
    alt="Bar chart showing the shared-memory CUDA kernel (0.553 s) beating the naive global-memory kernel (0.900 s) on an 8192×8192 matmul."
  />
</figure>

<h3>Benchmarking against PyTorch</h3>

<p>
  Lest you be impressed:
  <a
    href="https://github.com/SethHWeidman/ai_computing/tree/master/02_cuda_vs_pytorch_matmul"
    target="_blank"
    rel="noopener noreferrer"
    >this comparison</a
  >
  shows separately that the custom kernel shown here is about \(8 \times\) slower than
  just calling “<code>torch.mm</code>”! This is because <code>torch.mm</code> uses
  much-more-highly optimized CUDA kernels under-the-hood.
</p>

<figure>
  <img
    src="https://sethhweidman-personal-website.s3.us-east-1.amazonaws.com/2025-11-29_cuda-matmul/plots/02-cuda-vs-torch-timings.png"
    alt="Bar chart comparing the shared-memory CUDA kernel (8.32 ms) against torch.mm (1.11 ms) for a 2048×2048 matmul."
  />
</figure>

<h2>Conclusion</h2>

<p>
  This blog post covered some basics about CUDA programming, how those basics lead to a
  common pattern for writing "GPU-aware" code, and how that pattern can be used to speed
  up matrix multiplication. This pattern is very prevalent throughout AI coding; the
  widely-used FlashAttention technique uses a similar pattern to speed up the attention
  computation. Look for a deeper dive on FlashAttention in a future blog post!
</p>

<h3>Footnotes</h3>

<p id="fn1" class="footnote">
  <sup>
    <a href="#fnref1">1</a>
  </sup>
  There are lower levels of memory beyond the three listed here, but for the purposes of
  this blog post we focus on these, especially the distinction between global and shared
  memory.
  <a href="#fnref1" class="footnote-backref">↩</a>
</p>

<p id="fn2" class="footnote">
  <sup>
    <a href="#fnref2">2</a>
  </sup>
  Interestingly, CUDA leaves it up to the programmer, if operating on a matrix of total
  size \(X\), and wanting to use \(N\) threads per block, to compute the number of blocks
  of threads correctly as roughly \(\lceil X / N \rceil\).
  <a href="#fnref2" class="footnote-backref">↩</a>
</p>

<p id="fn3" class="footnote">
  <sup>
    <a href="#fnref3">3</a>
  </sup>
  To the best of my knowledge, it’s a coincidence that “shared memory” and “Streaming
  Multiprocessor” both have the initials “SM”. In CUDA-land, “SM” <em>always</em> refers
  to a “Streaming Multiprocessor”.
  <a href="#fnref3" class="footnote-backref">↩</a>
</p>

<p id="fn4" class="footnote">
  <sup>
    <a href="#fnref4">4</a>
  </sup>
  Roughly: a single float32 = 4 bytes. Thus, an 8B parameter model is 32 GB. The L4 GPU I
  used for these experiments has 24 GB of memory. Quantizing the float32 to float16 cuts
  the 32 GB down to 16 GB. For inference, this 16 GB would be all that is needed, meaning
  we <em>could</em> run inference with a 16 GB model on a single L4 GPU with
  quantization. For fine-tuning, some extra memory would be needed; and for a full
  training run, 2–4× the memory of this 16 GB would be needed (2x for the gradients
  alone).
  <a href="#fnref4" class="footnote-backref">↩</a>
</p>

<p id="fn5" class="footnote">
  <sup>
    <a href="#fnref5">5</a>
  </sup>
  Astute readers will ask:
  <em>why not load the entire matrices you're trying to multiply into shared memory</em>?
  The answer is that there usually simply isn't enough space. Each of the 58 SMs on an L4
  GPU
  <a
    href="https://docs.nvidia.com/cuda/archive/12.3.1/ada-tuning-guide/index.html#occupancy"
    target="_blank"
    rel="noopener noreferrer"
    >supports up to 100 KB of shared memory</a
  >. This means they could fit two 64 x 64 matrices (4 bytes * 64 * 64 * 2 is about 32
  KB), but even two 128 x 128 matrices (about 128 KB) would be too large. As the footnote
  above mentions, quantization changes this.
  <a href="#fnref5" class="footnote-backref">↩</a>
</p>
<br />
<button onclick="window.location.href='/'">Back to Home</button>
